{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Semantic Phasor Demo\n",
    "\n",
    "This notebook shows an interesting property of phasors when applied to textual data. We hope this will convince readers that this is a promising approach to creating vector representations of documents.\n",
    "\n",
    "## What the heck is a phasor? \n",
    "\n",
    "A phasor is just a complex number, but it has an interpretation built in: it represents *both* the amplitude and the phase of a sine wave of a fixed frequency. That is, it represents both how tall the wave is, and also where the wave starts (in the middle, up high, or down low).\n",
    "\n",
    "The output of a Fourier transform for a particular frequency is the same thing as a phasor. So in a sense \"doing stuff with phasors\" is just another way of saying \"Fourier analysis.\" But we talk about them here as phasors because it emphasizes this:\n",
    "\n",
    "### Phasors can be added and subtracted just like vectors. \n",
    "\n",
    "When you add two phasors for a given frequency together, you get a new phasor. It represents a wave with the same frequency, but with a different amplitude, and a different phase offset. There's a nice way of visualizing the geometry of this operation -- this is from the Wikipedia article on [phasors](https://en.wikipedia.org/wiki/Phasor): \n",
    "\n",
    "<img src=\"assets/Sumafasores.gif\" alt=\"An image showing the geometry of phasor addition.\" width=\"200px\" />\n",
    "\n",
    "The purple phasor is the sum of the red and blue phasors. Adding together wave signals seems a bit complicated, but the underlying geometry is the same as ordinary vector addition. It's just that the vectors are also rotating at a fixed rate around the origin.\n",
    "\n",
    "Because you can add and subtract phasors just like vectors, you can combine them with other vector-like objects -- including word embedding vectors. As long as the Fourier transform is run over dimensions that are orthogonal to one another, the resulting vectors are composable in the same way as word embeddings.\n",
    "\n",
    "## What does this notebook do with phasors?\n",
    "\n",
    "We wanted to see if there are any obvious patterns that phasors preserve. So we decided to see what happens when you \"rotate\" a text.\n",
    "\n",
    "Suppose you sliced off the first few hundred words of a text, tacked them on at the end, and then repeated that process until you were back where you began. It's as if you laid the words of the text out on a big wheel and started spinning it. Early rotations would start near the beginning of the original story, and end near the beginning again. Middle rotations would start in the middle of the original story, and end in the middle again. The very last one would be identical to the original.\n",
    "\n",
    "This notebook shows what happens if you take those rotated texts and do the following:\n",
    "\n",
    "1. Convert them into word vectors.\n",
    "2. Perform Fourier transforms over each dimension, and converting the resulting phasors into ordinary vectors, which represent whole documents.\n",
    "3. Pass the resulting document vectors to a UMAP model, which tries to preserve the topology of its input in a lower dimension.\n",
    "\n",
    "What will it look like? Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-214feb5bcb2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/th/lib/python3.9/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     )\n",
      "\u001b[0;32m~/.virtualenvs/th/lib/python3.9/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "en = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few imports, we load two files -- Frankenstein and Hamlet, both from Project Gutenberg -- and parse them using `spacy`. We use `spacy` because it provides pre-trained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('assets/frank.txt') as ip:\n",
    "    frank = ip.read()\n",
    "with open('assets/ham.txt') as ip:\n",
    "    ham = ip.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_sp = en(frank)\n",
    "ham_sp = en(ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some utility functions. \n",
    "\n",
    "- `text_vecs` takes the parsed text and generates the vectors, optionally rotating them by the number of words given by `offset`. \n",
    "- `group_sum` chunks the text up and sums the chunks, which makes the Fourier transform faster without losing too much accuracy. (Ideally, we'd like to do the transform over the raw data, but that takes a lot of time.)\n",
    "- `fft_vecs` takes the vectors for a text, performs the Fourier transform, and packages the result into one long vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vecs(text_sp, offset=0):\n",
    "    a = text_sp[offset:]\n",
    "    b = text_sp[:offset]\n",
    "    return numpy.array([t.vector.reshape(-1) for part in [a, b]\n",
    "                                             for t in part])\n",
    "\n",
    "def group_sum(vec, n_groups):\n",
    "    size = len(vec) / n_groups\n",
    "    ends = []\n",
    "    for i in range(1, n_groups + 1):\n",
    "        ends.append(int(size * i))\n",
    "    ends[-1] = len(vec)\n",
    "    \n",
    "    sums = []\n",
    "    start = 0\n",
    "    for e in ends:\n",
    "        sums.append(vec[start:e].sum())\n",
    "        start = e\n",
    "        \n",
    "    return numpy.array(sums)\n",
    "        \n",
    "def fft_vecs(vecs, n_bands=10):\n",
    "    fft_cols = []\n",
    "    n_groups = 1\n",
    "    while n_groups < n_bands * 4:\n",
    "        n_groups *= 2\n",
    "        \n",
    "    for col in range(vecs.shape[1]):\n",
    "        vec = vecs[:, col]\n",
    "        vec = group_sum(vec, n_groups)\n",
    "        fft = numpy.fft.rfft(vec)\n",
    "        fft_cols.append(fft[:n_bands])\n",
    "    return numpy.array(fft_cols).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick check to make sure the first function isn't doing something obviously wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_vecs = numpy.array([t.vector.reshape(-1) for t in frank_sp])\n",
    "assert (text_vecs(frank_sp, 0) == frank_vecs).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Now we generate the rotated word vectors. These are lists of matrices with 300 columns and VERYMANY rows, where VERYMANY is the number of words in the text. We're doing it in a slow way, so it takes a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frank_rotations = (text_vecs(frank_sp, offset) for offset in range(0, len(frank_sp), (len(frank_sp) // 100) + 1))\n",
    "ham_rotations = (text_vecs(ham_sp, offset) for offset in range(0, len(ham_sp), (len(ham_sp) // 100) + 1)) \n",
    "rotations = itertools.chain(frank_rotations, ham_rotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Now we do the fourier transforms. This also takes a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_vecs_fft = [fft_vecs(r) for r in rotations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to hold on to a copy of the original fourier transform data just in case, so we give it a new name for manipulation. If we ever want to reset anything below without having to re-do the fourier transforms, we can just run the below cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_vecs = rot_vecs_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we unpack the fourier transformed data into regular vectors. This means decomposing the complex numbers into pairs of regular floating point numbers. For our purposes, this is fine, because we aren't doing any multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rot_vecs = [[(x.real, x.imag) for x in arr] for arr in rot_vecs]\n",
    "rot_vecs = [[k for i_j in arr for k in i_j] for arr in rot_vecs]\n",
    "rot_vec_array = numpy.array(rot_vecs)\n",
    "rot_vec_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Finally, we pass the resulting document vectors to UMAP. To review, right now, we have 200 vectors of 6000 dimensions each. Each vector represents one document, and each document is a version of either *Frankenstein* or *Hamlet* rotated by some number of words.\n",
    "\n",
    "We want to see how `UMAP` lays out these vectors. Without getting into details, UMAP is a very useful algorithem that takes points in a high-dimensional space and tries to preserve the topology of those points in a low-dimensional space. This is different from more familiar alogirthms like t-SNE, which only try to preserve distance; UMAP actually assumes that the input data has an overall shape, and does its best to reproduce that same shape in a lower dimension.\n",
    "\n",
    "What shape will we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "um = umap.UMAP(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = um.fit_transform(rot_vec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gca().axis('equal')\n",
    "plt.scatter(vis[:, 0], \n",
    "            vis[:, 1], \n",
    "            c=[i / len(vis) for i in range(len(vis))],\n",
    "            cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP has done a great job of recreating the topology of the original data. The data was generated by rotation, and so it has given us circles!\n",
    "\n",
    "This doesn't prove that phasors will solve any real-world problems. But it does show that they do a good job of preserving certain information about the relative position of semantic peaks and troughs in documents. And it strongly suggests that they will help find duplicate documents without being overly sensitive to front- and end-matter. Since the overall peaks and troughs of two duplicates will be the same, any differences in offset will show up as small displacements around a circle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
