{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import textwrap\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import pandas\n",
    "import umap\n",
    "\n",
    "from headless import load_pages\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.palettes import magma\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_md', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BANDS = 50\n",
    "\n",
    "def htid_url(htid):\n",
    "    htid = htid.replace('+', ':').replace('=', '/')\n",
    "    return 'https://babel.hathitrust.org/cgi/pt?id={}'.format(htid)\n",
    "\n",
    "def path_to_htid(path):\n",
    "    htid = os.path.split(path)[-1]\n",
    "    htid = os.path.splitext(htid)[0]\n",
    "    return htid.replace('+', ':').replace('=', '/')\n",
    "\n",
    "def volume_paths(path):\n",
    "    \"\"\"List all zip files and subfolders in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if os.path.isdir(f) or f.endswith('.zip')]\n",
    "\n",
    "def numpy_paths(path):\n",
    "    \"\"\"List all numpy files in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if f.endswith('.npy')]\n",
    "\n",
    "def load_one_sp_embedding(volume_path):\n",
    "    \"\"\"Parse the text of one volume and extract word vectors.\"\"\"\n",
    "    sp_text = en_nlp.pipe(load_pages(volume_path))\n",
    "    return numpy.array([tok.vector.reshape(-1) for doc in sp_text for tok in doc if not tok.is_space])\n",
    "\n",
    "def piecewise_avg(vec, n_groups):\n",
    "    \"\"\"Divide a vector into pieces and return the average for each.\"\"\"\n",
    "    size = len(vec) / n_groups\n",
    "    ends = []\n",
    "    for i in range(1, n_groups + 1):\n",
    "        ends.append(int(size * i))\n",
    "    ends[-1] = len(vec)\n",
    "    \n",
    "    sums = []\n",
    "    start = 0\n",
    "    for end in ends:\n",
    "        sums.append(vec[start:end].sum() / (end - start))\n",
    "        start = end\n",
    "        \n",
    "    return numpy.array(sums)\n",
    "        \n",
    "def embedding_fft(sp_embedding, n_bands=N_BANDS):\n",
    "    \"\"\"\n",
    "    Perform a Fourier transform on all the dimensions of an\n",
    "    array of word embeddings extracted from a document. \n",
    "    `sp_embedding` is assumed to be an array with a row\n",
    "    for each document, and a column for each dimension of \n",
    "    the underlying word embedding vector model.\n",
    "    \"\"\"\n",
    "    fft_cols = []\n",
    "    n_groups = 1\n",
    "    while n_groups < n_bands * 10:\n",
    "        n_groups *= 2\n",
    "        \n",
    "    for col in range(sp_embedding.shape[1]):\n",
    "        vec = sp_embedding[:, col]\n",
    "        vec = piecewise_avg(vec, n_groups)\n",
    "        fft = numpy.fft.rfft(vec)\n",
    "        fft_cols.append(fft[:n_bands])\n",
    "    \n",
    "    return numpy.array(fft_cols)\n",
    "\n",
    "def flatten_fft(emb_fft, start=0, end=None, drop_zero_imag=False):\n",
    "    \"\"\"Reshape an fft array into a single vector.\"\"\"\n",
    "    complex_vec = numpy.array(emb_fft)[:, start:end].reshape(-1)\n",
    "    \n",
    "    # Check to see if all imaginary values are zero, and if so only include real\n",
    "    if drop_zero_imag and complex_vec.imag.ravel().sum() == 0:\n",
    "        return complex_vec.real\n",
    "    else:\n",
    "        return numpy.array([x for r_i in zip(complex_vec.real, complex_vec.imag)\n",
    "                            for x in r_i])\n",
    "\n",
    "def unflatten_vec(doc_vector):\n",
    "    \"\"\"Turn a document vector back into an fft array.\"\"\"\n",
    "    array = doc_vector.reshape(300, -1)  # This hard-codes values that should be parameters.\n",
    "    real = array[:, ::2]\n",
    "    imag = array[:, 1::2]\n",
    "    return real + imag * 1j\n",
    "\n",
    "def slice_vec_bands(doc_vectors, start=0, end=None):\n",
    "    return numpy.array([flatten_fft(unflatten_vec(dv), start, end, drop_zero_imag=True)\n",
    "                        for dv in doc_vectors])\n",
    "    \n",
    "def test_fft_reshape(volume_path):\n",
    "    \"\"\"A test of vector-array conversion routines.\"\"\"\n",
    "    assert _test_fft_reshape_one(volume_path)\n",
    "\n",
    "def _test_fft_reshape_one(folder):\n",
    "    emb = load_one_sp_embedding(folder)\n",
    "    \n",
    "    fft_orig = embedding_fft(emb)\n",
    "    \n",
    "    fft_complex = unflatten_vec(flatten_fft(fft_orig))\n",
    "    return (fft_orig == fft_complex).all()\n",
    "\n",
    "def save_embedding_ffts(source_path, dest_path=None):\n",
    "    dest_path = source_path if dest_path is None else dest_path\n",
    "    vol_paths = volume_paths(source_path)\n",
    "    vol_paths = [os.path.split(vp)[-1] for vp in vol_paths]\n",
    "    vol_paths = [vp if not vp.endswith('.zip') else vp[:-4] for vp in vol_paths]\n",
    "    new_paths = [os.path.join(dest_path, vp) for vp in vol_paths]\n",
    "    \n",
    "    sp_embeddings = (load_one_sp_embedding(v) for v in vol_paths)\n",
    "    emb_ffts = (embedding_fft(e) for e in sp_embeddings)\n",
    "    for emb, np in zip(emb_ffts, new_paths):\n",
    "        numpy.save(np, emb) \n",
    "    \n",
    "def load_embedding_fft_array(path, start=0, end=None, \n",
    "                             reload=False, htid_test=None, _cache={}):\n",
    "    if (reload or not _cache or _cache['start'] != start or \n",
    "                _cache['end'] != end or htid_test is not None):\n",
    "        if htid_test is not None:\n",
    "            assert [path_to_htid(p) for p in numpy_paths(path)] == list(htid_test)\n",
    "        _cache['start'] = start\n",
    "        _cache['end'] = end\n",
    "        _cache['data'] = numpy.array([flatten_fft(numpy.load(f), start, end) \n",
    "                                      for f in numpy_paths(path)])  \n",
    "    return _cache['data']\n",
    "\n",
    "# def load_embedding_fft_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "def load_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "    ids = [path_to_htid(p)\n",
    "           for p in numpy_paths(fft_path)]\n",
    "    metadata = (pandas\n",
    "                .read_csv(metadata_path, delimiter=csv_delim)\n",
    "                .drop_duplicates(htid_col)\n",
    "                .set_index(htid_col))\n",
    "    return metadata.reindex(ids, fill_value='[metadata missing]')\n",
    "\n",
    "def load_fft_metadata(fft_path, metadata_path, start=0, end=None, reload=False,\n",
    "                      csv_delim='\\t', htid_col='htid'):\n",
    "    metadata = load_metadata(metadata_path, fft_path, csv_delim, htid_col)\n",
    "    fft_arr = load_embedding_fft_array(fft_path, start, end, reload, metadata.index)\n",
    "    return fft_arr, metadata\n",
    "\n",
    "def deduplicator(data, **umap_kwargs):\n",
    "    data_umap = umap.UMAP(**umap_kwargs).fit_transform(data)\n",
    "    data_kd = KDTree(data_umap)\n",
    "    def deduplicate(distance):\n",
    "        pairs = data_kd.query_pairs(distance)\n",
    "        return set(frozenset(p) for p in pairs)\n",
    "    return deduplicate\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    return SequenceMatcher(a=a, b=b).ratio()\n",
    "\n",
    "def show_dataset(folder, n=10):\n",
    "    volumes = volume_paths(folder)\n",
    "    for v in volumes:\n",
    "        print(load_pages(v)[0][0:500])\n",
    "\n",
    "def show_umap(data, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    plt.gca().axis('equal')\n",
    "    plt.scatter(vis[:, 0], \n",
    "                vis[:, 1], \n",
    "                c=[i / len(vis) for i in range(len(vis))],\n",
    "                cmap='plasma')\n",
    "    plt.show()\n",
    "\n",
    "def umap_color(metadata, color_field, n_colors, dtype=None, palette=magma):\n",
    "    palette = palette(n_colors)\n",
    "    if color_field is None:\n",
    "        return [palette[n_colors // 2]] * len(metadata)\n",
    "    \n",
    "    if dtype is None:\n",
    "        dtype = type(metadata[color_field][0])\n",
    "    field = [f if isinstance(f, dtype) else dtype() \n",
    "             for f in metadata[color_field]]\n",
    "    field_rank = {f: i / len(field) for i, f in enumerate(sorted(field))}\n",
    "    return [palette[int(field_rank[f] * n_colors)] for f in field]\n",
    "\n",
    "def show_umap_bokeh(data, metadata, color_field=None, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    color = umap_color(metadata, color_field, 20, dtype=int)\n",
    "    scatter_data = pandas.DataFrame({'umap_1': vis[:, 0], \n",
    "                                     'umap_2': vis[:, 1],\n",
    "                                     'color': color,\n",
    "                                     'htid': list(metadata.index),\n",
    "                                     'title': ['<br>'.join(textwrap.wrap(t)) \n",
    "                                               for t in metadata['title']],\n",
    "                                     'author': list(metadata['author']),\n",
    "                                     'pub_date': list(metadata['pub_date'])\n",
    "    })\n",
    "    \n",
    "    plot_figure = figure(\n",
    "        title=('UMAP Projection of Phasor vectors for ~1000 '\n",
    "               'random HathiTrust volumes (colored by {})'.format(color_field)),\n",
    "        plot_width=800,\n",
    "        plot_height=800,\n",
    "        tools=('pan, wheel_zoom, reset')\n",
    "    )\n",
    "\n",
    "    plot_figure.add_tools(HoverTool(\n",
    "        tooltips=(\"<div><span style='font-size: 10px'>@htid{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@author{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@title{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@pub_date{safe}</span></div>\"\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    plot_figure.circle(\n",
    "        'umap_1',\n",
    "        'umap_2',\n",
    "        color='color',\n",
    "        source=scatter_data,\n",
    "    )\n",
    "\n",
    "    show(plot_figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_paths = ('/media/secure_volume/workset/orig',\n",
    "                '/media/secure_volume/workset/fft_npy',\n",
    "                '../fiction-rand-penn-1072.tsv')\n",
    "test_paths = ('../ht-open-test-data/gov_docs/',\n",
    "              '../ht-open-test-data/gov_docs_fft',\n",
    "              '../ht-open-test-data/gov_docs_fakemeta.csv')\n",
    "\n",
    "path_to_volumes, path_to_fft, path_to_meta = test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_path in volume_paths(path_to_volumes)[:3]:\n",
    "    print('testing with {}'.format(test_path))\n",
    "    try:\n",
    "        test_fft_reshape(test_path)\n",
    "    except Exception as e:\n",
    "        print('skipping {} -- {}: {}'.format(test_path, type(e), e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embedding_ffts(path_to_volumes, path_to_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = load_fft_metadata(path_to_fft, path_to_meta, end=20)\n",
    "show_umap_bokeh(\n",
    "    data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5\n",
    "dedupe_slice_full = deduplicator(slice_vec_bands(data, start=0, end=10),\n",
    "                                 n_neighbors=n_neighbors, n_components=10, metric='euclidean')\n",
    "dedupe_slice_1 = deduplicator(slice_vec_bands(data, start=0, end=1),\n",
    "                              n_neighbors=n_neighbors, n_components=10, metric='euclidean')\n",
    "dedupe_slice_2 = deduplicator(slice_vec_bands(data, start=1, end=2),\n",
    "                              n_neighbors=n_neighbors, n_components=10, metric='euclidean')\n",
    "dedupe_slice_3 = deduplicator(slice_vec_bands(data, start=2, end=3),\n",
    "                              n_neighbors=n_neighbors, n_components=10, metric='euclidean')\n",
    "dedupe_slice_4 = deduplicator(slice_vec_bands(data, start=3, end=4),\n",
    "                              n_neighbors=n_neighbors, n_components=10, metric='euclidean')\n",
    "dedupe_slice_5 = deduplicator(slice_vec_bands(data, start=4, end=5),\n",
    "                              n_neighbors=n_neighbors, n_components=10, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radius = 0.1\n",
    "pairs_boolean = (dedupe_slice_1(radius) & \n",
    "                  dedupe_slice_2(radius) & \n",
    "                  dedupe_slice_3(radius) & \n",
    "                  dedupe_slice_4(radius) & \n",
    "                  dedupe_slice_5(radius))\n",
    "pairs_single = dedupe_slice_1(radius)\n",
    "pairs_full = dedupe_slice_full(radius)\n",
    "print(\"Number of candidates found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(pairs_boolean))\n",
    "print(\"Single-band test: \", len(pairs_single))\n",
    "print(\"Full-band test:   \", len(pairs_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_true_positives_boolean = set(\n",
    "    frozenset((a, b)) for a, b in pairs_boolean\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "likely_true_positives_single = set(\n",
    "    frozenset((a, b)) for a, b in pairs_single\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "likely_true_positives_full = set(\n",
    "    frozenset((a, b)) for a, b in pairs_full\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "\n",
    "print(\"Number of near-certain duplicates (based on title) found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(likely_true_positives_boolean))\n",
    "print(\"Single-band test: \", len(likely_true_positives_single))\n",
    "print(\"Full-band test:   \", len(likely_true_positives_full))\n",
    "print()\n",
    "print(\"Number of near-certain duplicates missed by full test, caught by boolean test:\")\n",
    "print(len(likely_true_positives_boolean - likely_true_positives_full))\n",
    "print()\n",
    "print(\"Number of near-certain duplicates missed by boolean test, caught by full test:\")\n",
    "print(len(likely_true_positives_full - likely_true_positives_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(pairs_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [((data[x] - data[y]) ** 2).sum() ** 0.5 for x, y in pairs]\n",
    "order = sorted(range(len(distances)), key=distances.__getitem__)\n",
    "pairs_sorted = [pairs[i] for i in order]\n",
    "distances = [distances[i] for i in order]\n",
    "\n",
    "for a, b in pairs_sorted:\n",
    "    a_id = metadata.index[a]\n",
    "    b_id = metadata.index[b]\n",
    "    a_ti = metadata['title'][a]\n",
    "    b_ti = metadata['title'][b]\n",
    "    a_au = metadata['author'][a]\n",
    "    b_au = metadata['author'][b]\n",
    "    d = ((data[a] - data[b]) ** 2).sum() ** 0.5\n",
    "    s = string_similarity(a_ti, b_ti)\n",
    "    if s < 0.6:\n",
    "        continue\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print('Raw Phasor Embedding Distance: ', d)\n",
    "    print(htid_url(a_id))\n",
    "    print(a_au, \" ~~ \", a_ti)\n",
    "    print(htid_url(b_id))\n",
    "    print(b_au, \" ~~ \", b_ti)\n",
    "    print()\n",
    "    print('------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a, b in pair_ids:\n",
    "#     diff = data_dict[a] - data_dict[b]\n",
    "#     plt.plot(diff)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist between 10 smallest pairs in base space\n",
    "pair_dists = numpy.array([((data[x] - data[y]) ** 2).sum() ** 0.5 \n",
    "                          for x, y in pairs])\n",
    "smallest_ix = pair_dists.argsort()[:10]\n",
    "print(\"closest pairs in base space\")\n",
    "pprint(list(zip([pairs[i] for i in smallest_ix], pair_dists[smallest_ix])))\n",
    "\n",
    "# dist between pairs in umap space\n",
    "pair_umap_dists = numpy.array([((data_umap[x] - data_umap[y]) ** 2).sum() ** 0.5\n",
    "                               for x, y in pairs])\n",
    "smallest_ix = pair_umap_dists.argsort()[:10]\n",
    "print(\"closest pairs in umap space\")\n",
    "pprint(list(zip([pairs[i] for i in smallest_ix], pair_umap_dists[smallest_ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_freq_array = [unflatten_vec(r) / len(data) for r in data]  # 300 rows, 20 cols in each array, ~1000 arrays\n",
    "\n",
    "data_freq_mean = data_freq_array[0]\n",
    "for dfa in data_freq_array[1:]:\n",
    "    data_freq_mean += dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_a = data_freq_mean[:, 1:].real\n",
    "power_b = data_freq_mean[:, 1:].imag\n",
    "power = (power_a * power_a + power_b * power_b) ** 0.5\n",
    "\n",
    "mean_power = power.sum(axis=0) / 300\n",
    "plt.plot(mean_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_mean = mean_power / mean_power.mean()\n",
    "scaled_power = power / power.mean(axis=0)\n",
    "# scaled_rel_diff = []\n",
    "\n",
    "scaled_diffs = numpy.array([(((scaled_power[i] - scaled_mean) / scaled_power[i]) ** 2).sum()\n",
    "                            for i in range(len(power))])\n",
    "\n",
    "scaled_diffs_argsort = scaled_diffs.argsort()\n",
    "\n",
    "for chunk in range(10):\n",
    "    for i in range(chunk * 30, chunk * 30 + 30):\n",
    "        plt.plot(power[scaled_diffs_argsort[i]] / \n",
    "                 power[scaled_diffs_argsort[i]].mean())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
