{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import umap\n",
    "\n",
    "from headless import load_pages\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_md', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BANDS = 50\n",
    "\n",
    "def volume_paths(path):\n",
    "    \"\"\"List all zip files and subfolders in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if os.path.isdir(f) or f.endswith('.zip')]\n",
    "\n",
    "def numpy_paths(path):\n",
    "    \"\"\"List all numpy files in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if f.endswith('.npy')]\n",
    "\n",
    "def load_one_sp_embedding(volume_path):\n",
    "    \"\"\"Parse the text of one volume and extract word vectors.\"\"\"\n",
    "    sp_text = en_nlp.pipe(load_pages(volume_path))\n",
    "    return numpy.array([tok.vector.reshape(-1) for doc in sp_text for tok in doc])\n",
    "\n",
    "def piecewise_avg(vec, n_groups):\n",
    "    \"\"\"Divide a vector into pieces and return the average for each.\"\"\"\n",
    "    size = len(vec) / n_groups\n",
    "    ends = []\n",
    "    for i in range(1, n_groups + 1):\n",
    "        ends.append(int(size * i))\n",
    "    ends[-1] = len(vec)\n",
    "    \n",
    "    sums = []\n",
    "    start = 0\n",
    "    for end in ends:\n",
    "        sums.append(vec[start:end].sum() / (end - start))\n",
    "        start = end\n",
    "        \n",
    "    return numpy.array(sums)\n",
    "        \n",
    "def embedding_fft(sp_embedding, n_bands=N_BANDS, flatten=False):\n",
    "    \"\"\"\n",
    "    Perform a Fourier transform on all the dimensions of an\n",
    "    array of word embeddings extracted from a document. \n",
    "    `sp_embedding` is assumed to be an array with a row\n",
    "    for each document, and a column for each dimension of \n",
    "    the underlying word embedding vector model.\n",
    "    \"\"\"\n",
    "    fft_cols = []\n",
    "    n_groups = 1\n",
    "    while n_groups < n_bands * 10:\n",
    "        n_groups *= 2\n",
    "        \n",
    "    for col in range(sp_embedding.shape[1]):\n",
    "        vec = sp_embedding[:, col]\n",
    "        vec = piecewise_avg(vec, n_groups)\n",
    "        fft = numpy.fft.rfft(vec)\n",
    "        fft_cols.append(fft[:n_bands])\n",
    "    \n",
    "    if flatten:\n",
    "        return flatten_fft(fft_cols)\n",
    "    else:\n",
    "        return numpy.array(fft_cols)\n",
    "\n",
    "def flatten_fft(emb_fft):\n",
    "    \"\"\"Reshape an fft array into a single vector.\"\"\"\n",
    "    complex_vec = numpy.array(emb_fft).reshape(-1)\n",
    "    return numpy.array([x for r_i in zip(complex_vec.real, complex_vec.imag)\n",
    "                        for x in r_i])\n",
    "\n",
    "def unflatten_vec(doc_vector, cols=N_BANDS * 2):\n",
    "    \"\"\"Turn a document vector back into an fft array.\"\"\"\n",
    "    array = doc_vector.reshape(300, cols)  # This hard-codes values that should be parameters.\n",
    "    real = array[:, ::2]\n",
    "    imag = array[:, 1::2]\n",
    "    return real + imag * 1j\n",
    "    \n",
    "def test_fft_reshape(volume_path):\n",
    "    \"\"\"A test of vector-array conversion routines.\"\"\"\n",
    "    assert _test_fft_reshape_one(volume_path)\n",
    "\n",
    "def _test_fft_reshape_one(folder):\n",
    "    emb = load_one_sp_embedding(folder)\n",
    "    \n",
    "    fft_orig = embedding_fft(emb, flatten=False)\n",
    "    \n",
    "    fft_complex = unflatten_vec(embedding_fft(emb, flatten=True))\n",
    "    return (fft_orig == fft_complex).all()\n",
    "\n",
    "def vol_path_to_npy_path(vol_path):\n",
    "    return vol_path if not vol_path.endswith('.zip') else vol_path[:-4]\n",
    "\n",
    "def save_sp_embeddings(source_path, dest_path=None):\n",
    "    dest_path = source_path if dest_path is None else dest_path\n",
    "    vol_paths = volume_paths(source_path)\n",
    "    new_paths = [vol_path_to_npy_path(os.path.join(dest_path, os.path.split(vp)[-1]))\n",
    "                 for vp in vol_paths]\n",
    "    \n",
    "    sp_embeddings = (load_one_sp_embedding(v) for v in vol_paths)\n",
    "    for emb, np in zip(sp_embeddings, new_paths):\n",
    "        numpy.save(np, emb)        \n",
    "\n",
    "def save_embedding_ffts(source_path, dest_path=None):\n",
    "    dest_path = source_path if dest_path is None else dest_path\n",
    "    vol_paths = volume_paths(source_path)\n",
    "    new_paths = [vol_path_to_npy_path(os.path.join(dest_path, os.path.split(vp)[-1]))\n",
    "                 for vp in vol_paths]\n",
    "    \n",
    "    sp_embeddings = (load_one_sp_embedding(v) for v in vol_paths)\n",
    "    emb_ffts = (embedding_fft(e) for e in sp_embeddings)\n",
    "    for emb, np in zip(emb_ffts, new_paths):\n",
    "        numpy.save(np, emb) \n",
    "\n",
    "def load_embedding_fft_array(path):\n",
    "    return numpy.array([flatten_fft(numpy.load(f)) for f in numpy_paths(path)])\n",
    "\n",
    "def load_embedding_ffts(path, flatten=True):\n",
    "    if flatten:\n",
    "        return {os.path.split(f)[-1][:-4]: flatten_fft(numpy.load(f)) \n",
    "                for f in numpy_paths(path)}\n",
    "    else:\n",
    "        return {os.path.split(f)[-1][:-4]: numpy.load(f)\n",
    "                for f in numpy_paths(path)}\n",
    "\n",
    "def show_dataset(folder, n=None):\n",
    "    files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.txt')]\n",
    "    files.sort()\n",
    "    files = files[:n]\n",
    "    for f in files:\n",
    "        with open(f) as ip:\n",
    "            print(ip.read()[0:500])\n",
    "\n",
    "def show_umap(data):\n",
    "    print(data.shape)\n",
    "    um = umap.UMAP(n_neighbors=20, min_dist=0.001, metric='cosine')\n",
    "    vis = um.fit_transform(data)\n",
    "    plt.gca().axis('equal')\n",
    "    plt.scatter(vis[:, 0], \n",
    "                vis[:, 1], \n",
    "                c=[i / len(vis) for i in range(len(vis))],\n",
    "                cmap='plasma')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_volumes = '/media/secure_volume/workset/orig'\n",
    "path_to_volumes = '../ht-open-test-data/gov_docs/'\n",
    "path_to_fft = '../ht-open-test-data/gov_docs_fft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_path in volume_paths(path_to_volumes)[:2]:\n",
    "    print(test_path)\n",
    "    try:\n",
    "        test_fft_reshape(test_path)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embedding_ffts(path_to_volumes, path_to_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_umap(load_embedding_fft_array(path_to_fft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_embedding_ffts(path_to_fft)\n",
    "data = list(data_dict.items())\n",
    "files, data = zip(*data)\n",
    "data_umap = umap.UMAP(n_neighbors=5, n_components=10, metric='cosine').fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umap_kd = KDTree(data_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(data_umap_kd.query_pairs(0.02))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist between known duplicates:\n",
    "diff = data_dict['mdp.39015001704199'] - data_dict['mdp.39015048784154']\n",
    "print((diff * diff).sum() ** 0.5)\n",
    "plt.plot(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist between known duplicates:\n",
    "diff = data_dict['mdp.39015001704199'] - data_dict['nc01.ark+=13960=t2j68kf84']\n",
    "print((diff * diff).sum() ** 0.5)\n",
    "plt.plot(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist between random pair:\n",
    "diff = data_dict['mdp.39015001704199'] - data_dict['mdp.39015041912760']\n",
    "print((diff * diff).sum() ** 0.5)\n",
    "plt.plot(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'mdp.39015029726430', 'inu.39000000259478'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist between 10 smallest pairs in base space\n",
    "pair_dists = numpy.array([((data[x] - data[y]) ** 2).sum() ** 0.5 \n",
    "                          for x, y in pairs])\n",
    "smallest_ix = pair_dists.argsort()[:10]\n",
    "print(\"closest pairs in base space\")\n",
    "pprint(list(zip([pairs[i] for i in smallest_ix], pair_dists[smallest_ix])))\n",
    "\n",
    "# dist between pairs in umap space\n",
    "pair_umap_dists = [((data_umap[x] - data_umap[y]) ** 2).sum() ** 0.5 for x, y in pairs]\n",
    "smallest_ix = pair_dists.argsort()[:10]\n",
    "print(\"closest pairs in umap space\")\n",
    "pprint(list(zip([pairs[i] for i in smallest_ix], pair_dists[smallest_ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ids = [(files[x], files[y]) for x, y in pairs][:5]\n",
    "pair_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in pair_ids:\n",
    "    diff = data_dict[a] - data_dict[b]\n",
    "    plt.plot(diff)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_freq_array = [unflatten_vec(r) / len(data) for r in data]  # 300 rows, 20 cols in each array, ~1000 arrays\n",
    "data_freq_mean = data_freq_array[0]\n",
    "for dfa in data_freq_array[1:]:\n",
    "    data_freq_mean += dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_a = data_freq_mean[:, 2::2]\n",
    "power_b = data_freq_mean[:, 3::2]\n",
    "power = (power_a * power_a + power_b * power_b) ** 0.5\n",
    "\n",
    "mean_power = power.sum(axis=0) / 300\n",
    "plt.plot(mean_power)\n",
    "plt.plot(power[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(numpy.fft.ifft(mean_power)[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
