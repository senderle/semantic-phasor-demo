{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import pandas\n",
    "import umap\n",
    "\n",
    "from headless import load_pages\n",
    "from scipy.spatial import KDTree\n",
    "from pyhash import city_64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.palettes import magma\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_lg', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BANDS = 50\n",
    "\n",
    "def htid_url(htid):\n",
    "    htid = htid.replace('+', ':').replace('=', '/')\n",
    "    return 'https://babel.hathitrust.org/cgi/pt?id={}'.format(htid)\n",
    "\n",
    "def path_to_htid(path):\n",
    "    htid = os.path.split(path)[-1]\n",
    "    htid = os.path.splitext(htid)[0]\n",
    "    return htid.replace('+', ':').replace('=', '/')\n",
    "\n",
    "def volume_paths(path):\n",
    "    \"\"\"List all zip files and subfolders in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if os.path.isdir(f) or f.endswith('.zip')]\n",
    "\n",
    "def numpy_paths(path):\n",
    "    \"\"\"List all numpy files in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if f.endswith('.npy')]\n",
    "\n",
    "class VectorTable():\n",
    "    def __init__(self, spacy_model=None, ndims=300):\n",
    "        self._table = {}\n",
    "        self._doc_count = Counter()\n",
    "        \n",
    "        if spacy_model is None:\n",
    "            self._vec_table = self._srp_vec_table\n",
    "        else:\n",
    "            self._spacy_strings = spacy_model.vocab.strings\n",
    "            self._spacy_vectors = spacy_model.vocab.vectors\n",
    "            self._vec_table = self._sp_vec_table\n",
    "        \n",
    "        self.ndims = ndims\n",
    "    \n",
    "    def __getitem__(self, keys):\n",
    "        if isinstance(keys, str):\n",
    "            keys = [keys]\n",
    "        elif not isinstance(keys, list):\n",
    "            keys = list(keys)\n",
    "        \n",
    "        table = self._vec_table(keys)\n",
    "        result = numpy.empty((len(keys), self.ndims))\n",
    "        for i, k in enumerate(keys):\n",
    "            sub_index, sub_table = table[k]\n",
    "            result[i] = sub_table[sub_index]\n",
    "        return result\n",
    "    \n",
    "    def _sp_vec_table(self, keys):\n",
    "        keys = set(keys)\n",
    "        keys_ids = [(k, self._spacy_strings[k]) for k in keys\n",
    "                    if k in self._spacy_strings]\n",
    "        table = {k: (i, self._spacy_vectors) for k, i in keys_ids\n",
    "                 if i in self._spacy_vectors}\n",
    "        missing = keys - table.keys()\n",
    "        table.update(self._srp_vec_table(missing))\n",
    "        return table\n",
    "\n",
    "    def _srp_vec_table(self, keys):\n",
    "        keys = list(set(keys))\n",
    "        srp = self.srp_matrix(keys, self.ndims)\n",
    "        return {k: (i, srp) for i, k in enumerate(keys)}\n",
    "    \n",
    "    # This is a quick-and-dirty implementation of what Ben Schmidt calls\n",
    "    # \"Stable Random Projection.\" (Errors are mine, not his!)\n",
    "    @classmethod\n",
    "    def srp_matrix(cls, words, ndims, _hashfunc=city_64(0)):\n",
    "        multiplier = (ndims - 1) // 64 + 1\n",
    "        hashes = [\n",
    "            list(map(_hashfunc, ['{}_{}'.format(w, i) for i in range(multiplier)]))\n",
    "            for w in words\n",
    "        ]\n",
    "\n",
    "        # Given a `multipier` value of 5, `hashes` is really a Vx5\n",
    "        # array of 64-byte integers, where V is the vocabulary size\n",
    "        # and \n",
    "\n",
    "        hash_arr = numpy.array(hashes, dtype=numpy.uint64)\n",
    "\n",
    "        # But we could also think of it as an array of bytes,\n",
    "        # where every word is represented by 40 bytes...\n",
    "\n",
    "        hash_arr = hash_arr.view(dtype=numpy.uint8)\n",
    "\n",
    "        # ...or even as an array of bits, where every word is represented\n",
    "        # by 320 bits...\n",
    "\n",
    "        hash_arr = numpy.unpackbits(hash_arr.ravel()).reshape(-1, 64 * multiplier)\n",
    "\n",
    "        return (hash_arr.astype(numpy.float64) * 2 - 1)[:, :ndims]\n",
    "    \n",
    "def load_one_sp_embedding(volume_path, nlp=en_nlp, vec=VectorTable(spacy_model=en_nlp)):\n",
    "    \"\"\"Parse the text of one volume and extract word vectors.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    sp_text = nlp.pipe(load_pages(volume_path))\n",
    "    sp_text = [tok.lower_ for doc in sp_text for tok in doc if not tok.is_space]\n",
    "    print(\"parsing time:\", time.perf_counter() - start)\n",
    "    start = time.perf_counter()\n",
    "    res = vec[sp_text]\n",
    "    print(\"vectorizing time:\", time.perf_counter() - start)\n",
    "\n",
    "    return res\n",
    "\n",
    "def load_one_srp_embedding(volume_path, nlp=en_nlp, vec=VectorTable()):\n",
    "    \"\"\"Parse the text of one volume and extract word vectors.\"\"\"\n",
    "    sp_text = nlp.pipe(load_pages(volume_path))\n",
    "    return vec[[tok.lower_ for doc in sp_text for tok in doc if not tok.is_space]]\n",
    "                       \n",
    "def piecewise_avg(vec, n_groups):\n",
    "    \"\"\"Divide a vector into pieces and return the average for each.\"\"\"\n",
    "    size = len(vec) / n_groups\n",
    "    ends = []\n",
    "    for i in range(1, n_groups + 1):\n",
    "        ends.append(int(size * i))\n",
    "    ends[-1] = len(vec)\n",
    "    \n",
    "    sums = []\n",
    "    start = 0\n",
    "    for end in ends:\n",
    "        sums.append(vec[start:end].sum() / (end - start))\n",
    "        start = end\n",
    "        \n",
    "    return numpy.array(sums)\n",
    "        \n",
    "def embedding_fft(sp_embedding, n_bands=N_BANDS):\n",
    "    \"\"\"\n",
    "    Perform a Fourier transform on all the dimensions of an\n",
    "    array of word embeddings extracted from a document. \n",
    "    `sp_embedding` is assumed to be an array with a row\n",
    "    for each document, and a column for each dimension of \n",
    "    the underlying word embedding vector model.\n",
    "    \"\"\"\n",
    "    fft_cols = []\n",
    "    n_groups = 1\n",
    "    while n_groups < n_bands * 10:\n",
    "        n_groups *= 2\n",
    "        \n",
    "    for col in range(sp_embedding.shape[1]):\n",
    "        vec = sp_embedding[:, col]\n",
    "        vec = piecewise_avg(vec, n_groups)\n",
    "        fft = numpy.fft.rfft(vec)\n",
    "        fft_cols.append(fft[:n_bands])\n",
    "    \n",
    "    return numpy.array(fft_cols)\n",
    "\n",
    "def flatten_fft(emb_fft, start=0, end=None, drop_zero_imag=False):\n",
    "    \"\"\"Reshape an fft array into a single vector.\"\"\"\n",
    "    complex_vec = numpy.array(emb_fft)[:, start:end].reshape(-1)\n",
    "    \n",
    "    # Check to see if all imaginary values are zero, and if so only include real\n",
    "    if drop_zero_imag and complex_vec.imag.ravel().sum() == 0:\n",
    "        return complex_vec.real\n",
    "    else:\n",
    "        return numpy.array([x for r_i in zip(complex_vec.real, complex_vec.imag)\n",
    "                            for x in r_i])\n",
    "\n",
    "def unflatten_vec(doc_vector):\n",
    "    \"\"\"Turn a document vector back into an fft array.\"\"\"\n",
    "    array = doc_vector.reshape(300, -1)  # This hard-codes values that should be parameters.\n",
    "    real = array[:, ::2]\n",
    "    imag = array[:, 1::2]\n",
    "    return real + imag * 1j\n",
    "\n",
    "def slice_vec_bands(doc_vectors, start=0, end=None):\n",
    "    return numpy.array([flatten_fft(unflatten_vec(dv), start, end, drop_zero_imag=True)\n",
    "                        for dv in doc_vectors])\n",
    "    \n",
    "def test_fft_reshape(volume_path, srp=False):\n",
    "    \"\"\"A test of vector-array conversion routines.\"\"\"\n",
    "    assert _test_fft_reshape_one(volume_path, srp)\n",
    "\n",
    "def _test_fft_reshape_one(folder, srp):\n",
    "    if srp:\n",
    "        emb = load_one_srp_embedding(folder)\n",
    "    else:\n",
    "        emb = load_one_sp_embedding(folder)\n",
    "    \n",
    "    fft_orig = embedding_fft(emb)\n",
    "    \n",
    "    fft_complex = unflatten_vec(flatten_fft(fft_orig))\n",
    "    return (fft_orig == fft_complex).all()\n",
    "\n",
    "def save_embedding_ffts(source_path, dest_path=None, srp=False):\n",
    "    dest_path = source_path if dest_path is None else dest_path\n",
    "    vol_paths = volume_paths(source_path)\n",
    "    new_paths = [os.path.split(vp)[-1] for vp in vol_paths]\n",
    "    new_paths = [vp if not vp.endswith('.zip') else vp[:-4] for vp in new_paths]\n",
    "    new_paths = [os.path.join(dest_path, vp) for vp in new_paths]\n",
    "    \n",
    "    if srp:\n",
    "        load_emb = load_one_srp_embedding\n",
    "    else:\n",
    "        load_emb = load_one_sp_embedding\n",
    "\n",
    "    for vp, np in zip(vol_paths, new_paths):\n",
    "        if not os.path.exists(np + '.npy'):\n",
    "            numpy.save(np, embedding_fft(load_emb(vp)))  \n",
    "    \n",
    "def load_embedding_fft_array(path, start=0, end=None, \n",
    "                             reload=False, htid_test=None, _cache={}):\n",
    "    if (reload or not _cache or _cache['start'] != start or \n",
    "                _cache['end'] != end or htid_test is not None):\n",
    "        if htid_test is not None:\n",
    "            assert [path_to_htid(p) for p in numpy_paths(path)] == list(htid_test)\n",
    "        _cache['start'] = start\n",
    "        _cache['end'] = end\n",
    "        _cache['data'] = numpy.array([flatten_fft(numpy.load(f), start, end) \n",
    "                                      for f in numpy_paths(path)])  \n",
    "    return _cache['data']\n",
    "\n",
    "# def load_embedding_fft_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "def load_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "    ids = [path_to_htid(p)\n",
    "           for p in numpy_paths(fft_path)]\n",
    "    metadata = (pandas\n",
    "                .read_csv(metadata_path, delimiter=csv_delim)\n",
    "                .drop_duplicates(htid_col)\n",
    "                .set_index(htid_col))\n",
    "    return metadata.reindex(ids, fill_value='[metadata missing]')\n",
    "\n",
    "def load_fft_metadata(fft_path, metadata_path, start=0, end=None, reload=False,\n",
    "                      csv_delim='\\t', htid_col='htid'):\n",
    "    metadata = load_metadata(metadata_path, fft_path, csv_delim, htid_col)\n",
    "    fft_arr = load_embedding_fft_array(fft_path, start, end, reload, metadata.index)\n",
    "    return fft_arr, metadata\n",
    "\n",
    "def deduplicator(data, **umap_kwargs):\n",
    "    data_umap = umap.UMAP(**umap_kwargs).fit_transform(data)\n",
    "    data_kd = KDTree(data_umap)\n",
    "    def deduplicate(distance):\n",
    "        pairs = data_kd.query_pairs(distance)\n",
    "        return set(frozenset(p) for p in pairs)\n",
    "    return deduplicate\n",
    "\n",
    "def umap_concat(data, **umap_kwargs):\n",
    "    data_tiles = []\n",
    "    for i in range(5):\n",
    "        data_i = slice_vec_bands(data, start=i, end=i + 1)\n",
    "        data_tiles.append(umap.UMAP(**umap_kwargs).fit_transform(data))\n",
    "    data_concat = numpy.empty((data_tiles[0].shape[0], sum(dt.shape[1] for dt in data_tiles)))\n",
    "    start_col = 0\n",
    "    for dt in data_tiles:\n",
    "        end_col = start_col + dt.shape[1]\n",
    "        data[:, start_col:end_col] = dt\n",
    "        start_col = end_col\n",
    "        \n",
    "    return data_concat\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    return SequenceMatcher(a=a, b=b).ratio()\n",
    "\n",
    "def show_dataset(folder, n=10):\n",
    "    volumes = volume_paths(folder)\n",
    "    for v in volumes:\n",
    "        print(load_pages(v)[0][0:500])\n",
    "\n",
    "def show_umap(data, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    plt.gca().axis('equal')\n",
    "    plt.scatter(vis[:, 0], \n",
    "                vis[:, 1], \n",
    "                c=[i / len(vis) for i in range(len(vis))],\n",
    "                cmap='plasma')\n",
    "    plt.show()\n",
    "\n",
    "def umap_color(metadata, color_field, n_colors, dtype=None, palette=magma):\n",
    "    palette = palette(n_colors)\n",
    "    if color_field is None:\n",
    "        return [palette[n_colors // 2]] * len(metadata)\n",
    "    \n",
    "    if dtype is None:\n",
    "        dtype = type(metadata[color_field][0])\n",
    "    field = [f if isinstance(f, dtype) else dtype() \n",
    "             for f in metadata[color_field]]\n",
    "    n_colors = n_colors if len(set(field)) >= n_colors else len(set(field))\n",
    "    field_rank = {f: i / len(field) for i, f in enumerate(sorted(field))}\n",
    "    return [palette[int(field_rank[f] * n_colors)] for f in field]\n",
    "\n",
    "def show_umap_bokeh(data, metadata, color_field=None, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    color = umap_color(metadata, color_field, 20, dtype=int)\n",
    "    scatter_data = pandas.DataFrame({'umap_1': vis[:, 0], \n",
    "                                     'umap_2': vis[:, 1],\n",
    "                                     'color': color,\n",
    "                                     'htid': list(metadata.index),\n",
    "                                     'title': ['<br>'.join(textwrap.wrap(t)) \n",
    "                                               for t in metadata['title']],\n",
    "                                     'author': list(metadata['author']),\n",
    "                                     'pub_date': list(metadata['pub_date'])\n",
    "    })\n",
    "    \n",
    "    plot_figure = figure(\n",
    "        title=('UMAP Projection of Phasor vectors for ~1000 '\n",
    "               'random HathiTrust volumes (colored by {})'.format(color_field)),\n",
    "        plot_width=800,\n",
    "        plot_height=800,\n",
    "        tools=('pan, wheel_zoom, reset')\n",
    "    )\n",
    "\n",
    "    plot_figure.add_tools(HoverTool(\n",
    "        tooltips=(\"<div><span style='font-size: 10px'>@htid{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@author{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@title{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@pub_date{safe}</span></div>\"\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    plot_figure.circle(\n",
    "        'umap_1',\n",
    "        'umap_2',\n",
    "        color='color',\n",
    "        source=scatter_data,\n",
    "    )\n",
    "\n",
    "    show(plot_figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_paths = ('/media/secure_volume/workset/orig',\n",
    "                '/media/secure_volume/workset/fft_npy',\n",
    "                '/media/secure_volume/workset/srp_fft_npy',\n",
    "                '../fiction-rand-penn-1072.tsv')\n",
    "test_paths = ('../ht-open-test-data/gov_docs/',\n",
    "              '../ht-open-test-data/gov_docs_fft',\n",
    "              '../ht-open-test-data/gov_docs_srp_fft',\n",
    "              '../ht-open-test-data/gov_docs_fakemeta.csv')\n",
    "\n",
    "(path_to_volumes, \n",
    " path_to_fft, \n",
    " path_to_srp_fft, \n",
    " path_to_meta) = test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with ../ht-open-test-data/gov_docs/0314624,0001,001.zip\n",
      "parsing time: 0.30445615000002135\n",
      "vectorizing time: 0.10149922799996602\n",
      "testing with ../ht-open-test-data/gov_docs/0962976,1912,001.zip\n",
      "parsing time: 2.6592132419999643\n",
      "vectorizing time: 1.2054480009999793\n",
      "testing with ../ht-open-test-data/gov_docs/0962976,1912,002.zip\n",
      "parsing time: 2.9936590269999783\n",
      "vectorizing time: 1.5672972859999845\n"
     ]
    }
   ],
   "source": [
    "for test_path in volume_paths(path_to_volumes)[:3]:\n",
    "    print('testing with {}'.format(test_path))\n",
    "    try:\n",
    "        test_fft_reshape(test_path, srp=False)\n",
    "    except Exception as e:\n",
    "        print('skipping {} -- {}: {}'.format(test_path, type(e), e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing time: 0.143350909999981\n",
      "vectorizing time: 0.045309590999977445\n",
      "parsing time: 0.47625776000000997\n",
      "vectorizing time: 0.2201062770000135\n",
      "parsing time: 0.14011101700003792\n",
      "vectorizing time: 0.029446929000016553\n",
      "parsing time: 3.117928777999964\n",
      "vectorizing time: 1.3918283980000297\n",
      "parsing time: 5.054661702000033\n",
      "vectorizing time: 1.6431491849999702\n",
      "parsing time: 1.0577878770000098\n",
      "vectorizing time: 0.4825191940000195\n",
      "parsing time: 0.643033279000008\n",
      "vectorizing time: 0.22281498699999247\n",
      "parsing time: 1.4485138769999821\n",
      "vectorizing time: 0.471436261000008\n",
      "parsing time: 0.43629206999997905\n",
      "vectorizing time: 0.14361792599999035\n",
      "parsing time: 1.0644354190000058\n",
      "vectorizing time: 0.3993208389999836\n",
      "parsing time: 6.049080784000012\n",
      "vectorizing time: 2.4415389620000383\n",
      "parsing time: 0.4872278759999631\n",
      "vectorizing time: 0.15585157600003186\n",
      "parsing time: 0.2792201069999578\n",
      "vectorizing time: 0.07931508899997652\n",
      "parsing time: 0.09396643999997423\n",
      "vectorizing time: 0.023596165999947516\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-061bc0593335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_embedding_ffts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_volumes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_embedding_ffts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_volumes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_srp_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f1e64626a5cb>\u001b[0m in \u001b[0;36msave_embedding_ffts\u001b[0;34m(source_path, dest_path, srp)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvol_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m def load_embedding_fft_array(path, start=0, end=None, \n",
      "\u001b[0;32m<ipython-input-12-f1e64626a5cb>\u001b[0m in \u001b[0;36membedding_fft\u001b[0;34m(sp_embedding, n_bands)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpiecewise_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mfft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mfft_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_bands\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f1e64626a5cb>\u001b[0m in \u001b[0;36mpiecewise_avg\u001b[0;34m(vec, n_groups)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0msums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_embedding_ffts(path_to_volumes, path_to_fft, srp=False)\n",
    "save_embedding_ffts(path_to_volumes, path_to_srp_fft, srp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = load_fft_metadata(path_to_fft, path_to_meta, end=20)\n",
    "show_umap_bokeh(\n",
    "    slice_vec_bands(data, start=0, end=None),\n",
    "    metadata,\n",
    "    color_field='pub_date'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    n_neighbors=5, \n",
    "    n_components=10, \n",
    "    metric='euclidean'\n",
    ")\n",
    "dedupe_slice_full = deduplicator(slice_vec_bands(data, start=0, end=10), **kwargs)\n",
    "dedupe_slices = [deduplicator(slice_vec_bands(data, start=i, end=i + 1), **kwargs)\n",
    "                 for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radius = 0.8\n",
    "\n",
    "pairs_boolean = dedupe_slices[0](radius)\n",
    "for ds in dedupe_slices[1:]:\n",
    "    pairs_boolean &= ds(radius)\n",
    "\n",
    "pairs_single = dedupe_slices[0](radius)\n",
    "pairs_full = dedupe_slice_full(radius)\n",
    "print(\"Number of candidates found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(pairs_boolean))\n",
    "print(\"Single-band test: \", len(pairs_single))\n",
    "print(\"Full-band test:   \", len(pairs_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_true_positives_boolean = set(\n",
    "    frozenset((a, b)) for a, b in pairs_boolean\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "likely_true_positives_single = set(\n",
    "    frozenset((a, b)) for a, b in pairs_single\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "likely_true_positives_full = set(\n",
    "    frozenset((a, b)) for a, b in pairs_full\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "\n",
    "print(\"Number of likely duplicates (based on title) found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(likely_true_positives_boolean))\n",
    "print(\"Single-band test: \", len(likely_true_positives_single))\n",
    "print(\"Full-band test:   \", len(likely_true_positives_full))\n",
    "print()\n",
    "print(\"Number of near-certain duplicates missed by single test, caught by boolean test:\")\n",
    "print(len(likely_true_positives_boolean - likely_true_positives_single))\n",
    "print()\n",
    "print(\"Number of near-certain duplicates missed by boolean test, caught by single test:\")\n",
    "print(len(likely_true_positives_single - likely_true_positives_boolean))\n",
    "print()\n",
    "print(\"NOTE: A number of false positives may still appear in these counts because \"\n",
    "      \"different volumes from multi-volume works may have the same title even \"\n",
    "      \"though they do not contain the same content. This accounts for many of the \"\n",
    "      \"matches captured by the single-band but not by the boolean test. The \"\n",
    "      \"single-band test captures the broad semantic similarity between volumes of \"\n",
    "      \"the same work, but can't make fine-grained distinctions between individual \"\n",
    "      \"volumes of the work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(pairs_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [((data[x] - data[y]) ** 2).sum() ** 0.5 for x, y in pairs]\n",
    "order = sorted(range(len(distances)), key=distances.__getitem__)\n",
    "pairs_sorted = [pairs[i] for i in order]\n",
    "distances = [distances[i] for i in order]\n",
    "\n",
    "for a, b in pairs_sorted:\n",
    "    a_id = metadata.index[a]\n",
    "    b_id = metadata.index[b]\n",
    "    a_ti = metadata['title'][a]\n",
    "    b_ti = metadata['title'][b]\n",
    "    a_au = metadata['author'][a]\n",
    "    b_au = metadata['author'][b]\n",
    "    d = ((data[a] - data[b]) ** 2).sum() ** 0.5\n",
    "    s = string_similarity(a_ti, b_ti)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print('Raw Phasor Embedding Distance: ', d)\n",
    "    print(htid_url(a_id))\n",
    "    print(a_au, \" ~~ \", a_ti)\n",
    "    print(htid_url(b_id))\n",
    "    print(b_au, \" ~~ \", b_ti)\n",
    "    print()\n",
    "    print('------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_freq_array = [unflatten_vec(r) / len(data) for r in data]  # 300 rows, 20 cols in each array, ~1000 arrays\n",
    "\n",
    "data_freq_mean = data_freq_array[0]\n",
    "for dfa in data_freq_array[1:]:\n",
    "    data_freq_mean += dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_a = data_freq_mean[:, 1:].real\n",
    "power_b = data_freq_mean[:, 1:].imag\n",
    "power = (power_a * power_a + power_b * power_b) ** 0.5\n",
    "\n",
    "mean_power = power.sum(axis=0) / 300\n",
    "plt.plot(mean_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_mean = mean_power / mean_power.mean()\n",
    "scaled_power = power / power.mean(axis=0)\n",
    "# scaled_rel_diff = []\n",
    "\n",
    "scaled_diffs = numpy.array([(((scaled_power[i] - scaled_mean) / scaled_power[i]) ** 2).sum()\n",
    "                            for i in range(len(power))])\n",
    "\n",
    "scaled_diffs_argsort = scaled_diffs.argsort()\n",
    "\n",
    "for chunk in range(10):\n",
    "    for i in range(chunk * 30, chunk * 30 + 30):\n",
    "        plt.plot(power[scaled_diffs_argsort[i]] / \n",
    "                 power[scaled_diffs_argsort[i]].mean())\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable_vec = numpy.zeros(len(scaled_diffs), dtype=numpy.float64)\n",
    "unstable_vec[scaled_diffs_argsort] = (numpy.arange(len(scaled_diffs)) / len(scaled_diffs)) > 0.99\n",
    "unstable_vec = unstable_vec.reshape(1, -1)\n",
    "\n",
    "stable_vec = numpy.zeros(len(scaled_diffs), dtype=numpy.float64)\n",
    "stable_vec[scaled_diffs_argsort] = (numpy.arange(len(scaled_diffs)) / len(scaled_diffs)) < 0.01\n",
    "stable_vec = stable_vec.reshape(1, -1)\n",
    "\n",
    "def get_similar(vec, comp_word):\n",
    "    vec = vec + en_nlp.vocab.vectors[en_nlp.vocab.strings[comp_word]]\n",
    "    t_id = en_nlp.vocab.vectors.most_similar(vec)[0][0]\n",
    "    return en_nlp.vocab.strings[t_id]\n",
    "\n",
    "get_similar(unstable_vec, \"unexpected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
