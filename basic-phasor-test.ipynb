{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import textwrap\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import pandas\n",
    "import umap\n",
    "\n",
    "from headless import load_pages\n",
    "from scipy.spatial import KDTree\n",
    "from pyhash import city_64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.palettes import magma\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_lg', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BANDS = 50\n",
    "\n",
    "def htid_url(htid):\n",
    "    htid = htid.replace('+', ':').replace('=', '/')\n",
    "    return 'https://babel.hathitrust.org/cgi/pt?id={}'.format(htid)\n",
    "\n",
    "def path_to_htid(path):\n",
    "    htid = os.path.split(path)[-1]\n",
    "    htid = os.path.splitext(htid)[0]\n",
    "    return htid.replace('+', ':').replace('=', '/')\n",
    "\n",
    "def volume_paths(path):\n",
    "    \"\"\"List all zip files and subfolders in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if os.path.isdir(f) or f.endswith('.zip')]\n",
    "\n",
    "def numpy_paths(path):\n",
    "    \"\"\"List all numpy files in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if f.endswith('.npy')]\n",
    "\n",
    "class VectorTable():\n",
    "    def __init__(self, spacy_model=None, ndims=300):\n",
    "        self._table = {}\n",
    "        self._doc_count = Counter()\n",
    "        \n",
    "        if spacy_model is None:\n",
    "            self._update = self._update_from_srp\n",
    "        else:\n",
    "            self._spacy_strings = spacy_model.vocab.strings\n",
    "            self._spacy_vectors = spacy_model.vocab.vectors\n",
    "            self._update = self._update_from_spacy\n",
    "        \n",
    "        self.ndims = ndims\n",
    "    \n",
    "    def __getitem__(self, keys):\n",
    "        if isinstance(keys, str):\n",
    "            keys = [keys]\n",
    "        elif not isinstance(keys, list):\n",
    "            keys = list(keys)\n",
    "        \n",
    "        self.update(keys)\n",
    "        result = numpy.array([self._table[k] for k in keys])\n",
    "        return result\n",
    "    \n",
    "    def update(self, newkeyset):\n",
    "        if len(self._table) > 4000000:\n",
    "            self._doc_count = Counter(dict(self._doc_count.most_common(500000)))\n",
    "            self._table = {k: self._table[k] for k in self._doc_count}\n",
    "        newkeyset = set(newkeyset)\n",
    "        self._doc_count.update(newkeyset)\n",
    "        self._update(newkeyset)\n",
    "    \n",
    "    def _update_from_spacy(self, newkeyset):\n",
    "        keys_ids = [(k, self._spacy_strings[k]) for k in newkeyset\n",
    "                    if k in self._spacy_strings]\n",
    "        self._table.update({k: self._spacy_vectors[i] for k, i in keys_ids\n",
    "                            if i in self._spacy_vectors})\n",
    "        missing = newkeyset - self._table.keys()\n",
    "        self._update_from_srp(missing)\n",
    "    \n",
    "    def _update_from_srp(self, newkeyset):\n",
    "        newkeyset = list(newkeyset)\n",
    "        srp = self.srp_matrix(newkeyset, self.ndims)\n",
    "        self._table.update({k: vec for k, vec in zip(newkeyset, srp)})\n",
    "\n",
    "    # This is a quick-and-dirty implementation of what Ben Schmidt calls\n",
    "    # \"Stable Random Projection.\" (Errors are mine, not his!)\n",
    "    @classmethod\n",
    "    def srp_matrix(cls, words, ndims, _hashfunc=city_64(0)):\n",
    "        multiplier = (ndims - 1) // 64 + 1\n",
    "        hashes = [\n",
    "            list(map(_hashfunc, ['{}_{}'.format(w, i) for i in range(multiplier)]))\n",
    "            for w in words\n",
    "        ]\n",
    "\n",
    "        # Given a `multipier` value of 5, `hashes` is really a Vx5\n",
    "        # array of 64-byte integers, where V is the vocabulary size\n",
    "        # and \n",
    "\n",
    "        hash_arr = numpy.array(hashes, dtype=numpy.uint64)\n",
    "\n",
    "        # But we could also think of it as an array of bytes,\n",
    "        # where every word is represented by 40 bytes...\n",
    "\n",
    "        hash_arr = hash_arr.view(dtype=numpy.uint8)\n",
    "\n",
    "        # ...or even as an array of bits, where every word is represented\n",
    "        # by 320 bits...\n",
    "\n",
    "        hash_arr = numpy.unpackbits(hash_arr.ravel()).reshape(-1, 64 * multiplier)\n",
    "\n",
    "        return (hash_arr.astype(numpy.float64) * 2 - 1)[:, :ndims]\n",
    "    \n",
    "def load_one_sp_embedding(volume_path, nlp=en_nlp, vec=VectorTable(spacy_model=en_nlp)):\n",
    "    \"\"\"Parse the text of one volume and extract word vectors.\"\"\"\n",
    "    sp_text = nlp.pipe(load_pages(volume_path))\n",
    "    return vec[[tok.lower_ for doc in sp_text for tok in doc if not tok.is_space]]\n",
    "\n",
    "def load_one_srp_embedding(volume_path, nlp=en_nlp, vec=VectorTable()):\n",
    "    \"\"\"Parse the text of one volume and extract word vectors.\"\"\"\n",
    "    sp_text = nlp.pipe(load_pages(volume_path))\n",
    "    return vec[[tok.lower_ for doc in sp_text for tok in doc if not tok.is_space]]\n",
    "                       \n",
    "def piecewise_avg(vec, n_groups):\n",
    "    \"\"\"Divide a vector into pieces and return the average for each.\"\"\"\n",
    "    size = len(vec) / n_groups\n",
    "    ends = []\n",
    "    for i in range(1, n_groups + 1):\n",
    "        ends.append(int(size * i))\n",
    "    ends[-1] = len(vec)\n",
    "    \n",
    "    sums = []\n",
    "    start = 0\n",
    "    for end in ends:\n",
    "        sums.append(vec[start:end].sum() / (end - start))\n",
    "        start = end\n",
    "        \n",
    "    return numpy.array(sums)\n",
    "        \n",
    "def embedding_fft(sp_embedding, n_bands=N_BANDS):\n",
    "    \"\"\"\n",
    "    Perform a Fourier transform on all the dimensions of an\n",
    "    array of word embeddings extracted from a document. \n",
    "    `sp_embedding` is assumed to be an array with a row\n",
    "    for each document, and a column for each dimension of \n",
    "    the underlying word embedding vector model.\n",
    "    \"\"\"\n",
    "    fft_cols = []\n",
    "    n_groups = 1\n",
    "    while n_groups < n_bands * 10:\n",
    "        n_groups *= 2\n",
    "        \n",
    "    for col in range(sp_embedding.shape[1]):\n",
    "        vec = sp_embedding[:, col]\n",
    "        vec = piecewise_avg(vec, n_groups)\n",
    "        fft = numpy.fft.rfft(vec)\n",
    "        fft_cols.append(fft[:n_bands])\n",
    "    \n",
    "    return numpy.array(fft_cols)\n",
    "\n",
    "def flatten_fft(emb_fft, start=0, end=None, drop_zero_imag=False):\n",
    "    \"\"\"Reshape an fft array into a single vector.\"\"\"\n",
    "    complex_vec = numpy.array(emb_fft)[:, start:end].reshape(-1)\n",
    "    \n",
    "    # Check to see if all imaginary values are zero, and if so only include real\n",
    "    if drop_zero_imag and complex_vec.imag.ravel().sum() == 0:\n",
    "        return complex_vec.real\n",
    "    else:\n",
    "        return numpy.array([x for r_i in zip(complex_vec.real, complex_vec.imag)\n",
    "                            for x in r_i])\n",
    "\n",
    "def unflatten_vec(doc_vector):\n",
    "    \"\"\"Turn a document vector back into an fft array.\"\"\"\n",
    "    array = doc_vector.reshape(300, -1)  # This hard-codes values that should be parameters.\n",
    "    real = array[:, ::2]\n",
    "    imag = array[:, 1::2]\n",
    "    return real + imag * 1j\n",
    "\n",
    "def slice_vec_bands(doc_vectors, start=0, end=None):\n",
    "    return numpy.array([flatten_fft(unflatten_vec(dv), start, end, drop_zero_imag=True)\n",
    "                        for dv in doc_vectors])\n",
    "    \n",
    "def test_fft_reshape(volume_path, srp=False):\n",
    "    \"\"\"A test of vector-array conversion routines.\"\"\"\n",
    "    assert _test_fft_reshape_one(volume_path, srp)\n",
    "\n",
    "def _test_fft_reshape_one(folder, srp):\n",
    "    if srp:\n",
    "        emb = load_one_srp_embedding(folder)\n",
    "    else:\n",
    "        emb = load_one_sp_embedding(folder)\n",
    "    \n",
    "    fft_orig = embedding_fft(emb)\n",
    "    \n",
    "    fft_complex = unflatten_vec(flatten_fft(fft_orig))\n",
    "    return (fft_orig == fft_complex).all()\n",
    "\n",
    "def save_embedding_ffts(source_path, dest_path=None, srp=False):\n",
    "    dest_path = source_path if dest_path is None else dest_path\n",
    "    vol_paths = volume_paths(source_path)\n",
    "    new_paths = [os.path.split(vp)[-1] for vp in vol_paths]\n",
    "    new_paths = [vp if not vp.endswith('.zip') else vp[:-4] for vp in new_paths]\n",
    "    new_paths = [os.path.join(dest_path, vp) for vp in new_paths]\n",
    "    \n",
    "    if srp:\n",
    "        load_emb = load_one_srp_embedding\n",
    "    else:\n",
    "        load_emb = load_one_sp_embedding\n",
    "\n",
    "    for vp, np in zip(vol_paths, new_paths):\n",
    "        if not os.path.exists(np + '.npy'):\n",
    "            numpy.save(np, embedding_fft(load_emb(vp)))  \n",
    "    \n",
    "def load_embedding_fft_array(path, start=0, end=None, \n",
    "                             reload=False, htid_test=None, _cache={}):\n",
    "    if (reload or not _cache or _cache['start'] != start or \n",
    "                _cache['end'] != end or htid_test is not None):\n",
    "        if htid_test is not None:\n",
    "            assert [path_to_htid(p) for p in numpy_paths(path)] == list(htid_test)\n",
    "        _cache['start'] = start\n",
    "        _cache['end'] = end\n",
    "        _cache['data'] = numpy.array([flatten_fft(numpy.load(f), start, end) \n",
    "                                      for f in numpy_paths(path)])  \n",
    "    return _cache['data']\n",
    "\n",
    "# def load_embedding_fft_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "def load_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "    ids = [path_to_htid(p)\n",
    "           for p in numpy_paths(fft_path)]\n",
    "    metadata = (pandas\n",
    "                .read_csv(metadata_path, delimiter=csv_delim)\n",
    "                .drop_duplicates(htid_col)\n",
    "                .set_index(htid_col))\n",
    "    return metadata.reindex(ids, fill_value='[metadata missing]')\n",
    "\n",
    "def load_fft_metadata(fft_path, metadata_path, start=0, end=None, reload=False,\n",
    "                      csv_delim='\\t', htid_col='htid'):\n",
    "    metadata = load_metadata(metadata_path, fft_path, csv_delim, htid_col)\n",
    "    fft_arr = load_embedding_fft_array(fft_path, start, end, reload, metadata.index)\n",
    "    return fft_arr, metadata\n",
    "\n",
    "def deduplicator(data, **umap_kwargs):\n",
    "    data_umap = umap.UMAP(**umap_kwargs).fit_transform(data)\n",
    "    data_kd = KDTree(data_umap)\n",
    "    def deduplicate(distance):\n",
    "        pairs = data_kd.query_pairs(distance)\n",
    "        return set(frozenset(p) for p in pairs)\n",
    "    return deduplicate\n",
    "\n",
    "def umap_concat(data, **umap_kwargs):\n",
    "    data_tiles = []\n",
    "    for i in range(5):\n",
    "        data_i = slice_vec_bands(data, start=i, end=i + 1)\n",
    "        data_tiles.append(umap.UMAP(**umap_kwargs).fit_transform(data))\n",
    "    data_concat = numpy.empty((data_tiles[0].shape[0], sum(dt.shape[1] for dt in data_tiles)))\n",
    "    start_col = 0\n",
    "    for dt in data_tiles:\n",
    "        end_col = start_col + dt.shape[1]\n",
    "        data[:, start_col:end_col] = dt\n",
    "        start_col = end_col\n",
    "        \n",
    "    return data_concat\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    return SequenceMatcher(a=a, b=b).ratio()\n",
    "\n",
    "def show_dataset(folder, n=10):\n",
    "    volumes = volume_paths(folder)\n",
    "    for v in volumes:\n",
    "        print(load_pages(v)[0][0:500])\n",
    "\n",
    "def show_umap(data, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    plt.gca().axis('equal')\n",
    "    plt.scatter(vis[:, 0], \n",
    "                vis[:, 1], \n",
    "                c=[i / len(vis) for i in range(len(vis))],\n",
    "                cmap='plasma')\n",
    "    plt.show()\n",
    "\n",
    "def umap_color(metadata, color_field, n_colors, dtype=None, palette=magma):\n",
    "    palette = palette(n_colors)\n",
    "    if color_field is None:\n",
    "        return [palette[n_colors // 2]] * len(metadata)\n",
    "    \n",
    "    if dtype is None:\n",
    "        dtype = type(metadata[color_field][0])\n",
    "    field = [f if isinstance(f, dtype) else dtype() \n",
    "             for f in metadata[color_field]]\n",
    "    n_colors = n_colors if len(set(field)) >= n_colors else len(set(field))\n",
    "    field_rank = {f: i / len(field) for i, f in enumerate(sorted(field))}\n",
    "    return [palette[int(field_rank[f] * n_colors)] for f in field]\n",
    "\n",
    "def show_umap_bokeh(data, metadata, color_field=None, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    color = umap_color(metadata, color_field, 20, dtype=int)\n",
    "    scatter_data = pandas.DataFrame({'umap_1': vis[:, 0], \n",
    "                                     'umap_2': vis[:, 1],\n",
    "                                     'color': color,\n",
    "                                     'htid': list(metadata.index),\n",
    "                                     'title': ['<br>'.join(textwrap.wrap(t)) \n",
    "                                               for t in metadata['title']],\n",
    "                                     'author': list(metadata['author']),\n",
    "                                     'pub_date': list(metadata['pub_date'])\n",
    "    })\n",
    "    \n",
    "    plot_figure = figure(\n",
    "        title=('UMAP Projection of Phasor vectors for ~1000 '\n",
    "               'random HathiTrust volumes (colored by {})'.format(color_field)),\n",
    "        plot_width=800,\n",
    "        plot_height=800,\n",
    "        tools=('pan, wheel_zoom, reset')\n",
    "    )\n",
    "\n",
    "    plot_figure.add_tools(HoverTool(\n",
    "        tooltips=(\"<div><span style='font-size: 10px'>@htid{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@author{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@title{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@pub_date{safe}</span></div>\"\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    plot_figure.circle(\n",
    "        'umap_1',\n",
    "        'umap_2',\n",
    "        color='color',\n",
    "        source=scatter_data,\n",
    "    )\n",
    "\n",
    "    show(plot_figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_paths = ('/media/secure_volume/workset/orig',\n",
    "                '/media/secure_volume/workset/fft_npy',\n",
    "                '/media/secure_volume/workset/srp_fft_npy',\n",
    "                '../fiction-rand-penn-1072.tsv')\n",
    "test_paths = ('../ht-open-test-data/gov_docs/',\n",
    "              '../ht-open-test-data/gov_docs_fft',\n",
    "              '../ht-open-test-data/gov_docs_srp_fft',\n",
    "              '../ht-open-test-data/gov_docs_fakemeta.csv')\n",
    "\n",
    "(path_to_volumes, \n",
    " path_to_fft, \n",
    " path_to_srp_fft, \n",
    " path_to_meta) = test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_path in volume_paths(path_to_volumes)[:3]:\n",
    "    print('testing with {}'.format(test_path))\n",
    "    try:\n",
    "        test_fft_reshape(test_path, srp=False)\n",
    "    except Exception as e:\n",
    "        print('skipping {} -- {}: {}'.format(test_path, type(e), e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding_ffts(path_to_volumes, path_to_fft, srp=False)\n",
    "save_embedding_ffts(path_to_volumes, path_to_srp_fft, srp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = load_fft_metadata(path_to_fft, path_to_meta, end=20)\n",
    "show_umap_bokeh(\n",
    "    slice_vec_bands(data, start=0, end=None),\n",
    "    metadata,\n",
    "    color_field='pub_date'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    n_neighbors=5, \n",
    "    n_components=10, \n",
    "    metric='euclidean'\n",
    ")\n",
    "dedupe_slice_full = deduplicator(slice_vec_bands(data, start=0, end=10), **kwargs)\n",
    "dedupe_slices = [deduplicator(slice_vec_bands(data, start=i, end=i + 1), **kwargs)\n",
    "                 for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radius = 0.8\n",
    "\n",
    "pairs_boolean = dedupe_slices[0](radius)\n",
    "for ds in dedupe_slices[1:]:\n",
    "    pairs_boolean &= ds(radius)\n",
    "\n",
    "pairs_single = dedupe_slices[0](radius)\n",
    "pairs_full = dedupe_slice_full(radius)\n",
    "print(\"Number of candidates found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(pairs_boolean))\n",
    "print(\"Single-band test: \", len(pairs_single))\n",
    "print(\"Full-band test:   \", len(pairs_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_true_positives_boolean = set(\n",
    "    frozenset((a, b)) for a, b in pairs_boolean\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "likely_true_positives_single = set(\n",
    "    frozenset((a, b)) for a, b in pairs_single\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "likely_true_positives_full = set(\n",
    "    frozenset((a, b)) for a, b in pairs_full\n",
    "    if string_similarity(metadata['title'][a], metadata['title'][b]) >= 0.95\n",
    ")\n",
    "\n",
    "print(\"Number of likely duplicates (based on title) found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(likely_true_positives_boolean))\n",
    "print(\"Single-band test: \", len(likely_true_positives_single))\n",
    "print(\"Full-band test:   \", len(likely_true_positives_full))\n",
    "print()\n",
    "print(\"Number of near-certain duplicates missed by single test, caught by boolean test:\")\n",
    "print(len(likely_true_positives_boolean - likely_true_positives_single))\n",
    "print()\n",
    "print(\"Number of near-certain duplicates missed by boolean test, caught by single test:\")\n",
    "print(len(likely_true_positives_single - likely_true_positives_boolean))\n",
    "print()\n",
    "print(\"NOTE: A number of false positives may still appear in these counts because \"\n",
    "      \"different volumes from multi-volume works may have the same title even \"\n",
    "      \"though they do not contain the same content. This accounts for many of the \"\n",
    "      \"matches captured by the single-band but not by the boolean test. The \"\n",
    "      \"single-band test captures the broad semantic similarity between volumes of \"\n",
    "      \"the same work, but can't make fine-grained distinctions between individual \"\n",
    "      \"volumes of the work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(pairs_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [((data[x] - data[y]) ** 2).sum() ** 0.5 for x, y in pairs]\n",
    "order = sorted(range(len(distances)), key=distances.__getitem__)\n",
    "pairs_sorted = [pairs[i] for i in order]\n",
    "distances = [distances[i] for i in order]\n",
    "\n",
    "for a, b in pairs_sorted:\n",
    "    a_id = metadata.index[a]\n",
    "    b_id = metadata.index[b]\n",
    "    a_ti = metadata['title'][a]\n",
    "    b_ti = metadata['title'][b]\n",
    "    a_au = metadata['author'][a]\n",
    "    b_au = metadata['author'][b]\n",
    "    d = ((data[a] - data[b]) ** 2).sum() ** 0.5\n",
    "    s = string_similarity(a_ti, b_ti)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print('Raw Phasor Embedding Distance: ', d)\n",
    "    print(htid_url(a_id))\n",
    "    print(a_au, \" ~~ \", a_ti)\n",
    "    print(htid_url(b_id))\n",
    "    print(b_au, \" ~~ \", b_ti)\n",
    "    print()\n",
    "    print('------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_freq_array = [unflatten_vec(r) / len(data) for r in data]  # 300 rows, 20 cols in each array, ~1000 arrays\n",
    "\n",
    "data_freq_mean = data_freq_array[0]\n",
    "for dfa in data_freq_array[1:]:\n",
    "    data_freq_mean += dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_a = data_freq_mean[:, 1:].real\n",
    "power_b = data_freq_mean[:, 1:].imag\n",
    "power = (power_a * power_a + power_b * power_b) ** 0.5\n",
    "\n",
    "mean_power = power.sum(axis=0) / 300\n",
    "plt.plot(mean_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_mean = mean_power / mean_power.mean()\n",
    "scaled_power = power / power.mean(axis=0)\n",
    "# scaled_rel_diff = []\n",
    "\n",
    "scaled_diffs = numpy.array([(((scaled_power[i] - scaled_mean) / scaled_power[i]) ** 2).sum()\n",
    "                            for i in range(len(power))])\n",
    "\n",
    "scaled_diffs_argsort = scaled_diffs.argsort()\n",
    "\n",
    "for chunk in range(10):\n",
    "    for i in range(chunk * 30, chunk * 30 + 30):\n",
    "        plt.plot(power[scaled_diffs_argsort[i]] / \n",
    "                 power[scaled_diffs_argsort[i]].mean())\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable_vec = numpy.zeros(len(scaled_diffs), dtype=numpy.float64)\n",
    "unstable_vec[scaled_diffs_argsort] = (numpy.arange(len(scaled_diffs)) / len(scaled_diffs)) > 0.99\n",
    "unstable_vec = unstable_vec.reshape(1, -1)\n",
    "\n",
    "stable_vec = numpy.zeros(len(scaled_diffs), dtype=numpy.float64)\n",
    "stable_vec[scaled_diffs_argsort] = (numpy.arange(len(scaled_diffs)) / len(scaled_diffs)) < 0.01\n",
    "stable_vec = stable_vec.reshape(1, -1)\n",
    "\n",
    "def get_similar(vec, comp_word):\n",
    "    vec = vec + en_nlp.vocab.vectors[en_nlp.vocab.strings[comp_word]]\n",
    "    t_id = en_nlp.vocab.vectors.most_similar(vec)[0][0]\n",
    "    return en_nlp.vocab.strings[t_id]\n",
    "\n",
    "get_similar(unstable_vec, \"unexpected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
