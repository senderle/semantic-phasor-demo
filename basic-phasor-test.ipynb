{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import textwrap\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "from pprint import pprint\n",
    "from collections import Counter, deque\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import pandas\n",
    "import umap\n",
    "\n",
    "import phasor\n",
    "\n",
    "from headless import load_pages\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.neighbors import BallTree\n",
    "from pyhash import city_64\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, TapTool, OpenURL, ColumnDataSource\n",
    "from bokeh.palettes import magma\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'test-batch'\n",
    "dataset = 'rand-fiction-1k'\n",
    "\n",
    "secure_paths = (f'/media/secure_volume/volumes/zip/{dataset}',\n",
    "                f'/media/secure_volume/derived/{dataset}/fft',\n",
    "                f'/media/secure_volume/derived/{dataset}/srp_fft',\n",
    "                f'/media/secure_volume/worksets/{dataset}-hathifiles.csv')\n",
    "\n",
    "test_paths = ('../ht-open-test-data/fiction_998/',\n",
    "              '../ht-open-test-data/fiction_fft',\n",
    "              '../ht-open-test-data/fiction_srp_fft',\n",
    "              '../ht-open-test-data/fiction.csv')\n",
    "\n",
    "(path_to_volumes, \n",
    " path_to_fft, \n",
    " path_to_srp_fft, \n",
    " path_to_meta) = test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test_path in phasor.volume_paths(path_to_volumes)[:3]:\n",
    "#     print('testing with {}'.format(test_path))\n",
    "#     try:\n",
    "#         phasor.test_fft_reshape(test_path, srp=False)\n",
    "#     except Exception as e:\n",
    "#         print('skipping {} -- {}: {}'.format(test_path, type(e), e))\n",
    "\n",
    "# for test_path in phasor.volume_paths(path_to_volumes):\n",
    "#     phasor.test_htid_conversion(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = phasor.save_embedding_ffts(path_to_volumes, path_to_fft, srp=False)\n",
    "paths = phasor.save_embedding_ffts(path_to_volumes, path_to_srp_fft, srp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = phasor.load_fft_metadata(path_to_fft, path_to_meta, end=20, csv_delim=',')\n",
    "data[numpy.isnan(data)] = 0\n",
    "# phasor.show_umap_bokeh(\n",
    "#     phasor.slice_vec_bands(data, start=0, end=1),\n",
    "#     metadata,\n",
    "#     n_neighbors=50\n",
    "#     # color_field='pub_date'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    n_neighbors=10, \n",
    "    n_components=5, \n",
    "    metric='euclidean',\n",
    "    # random=True\n",
    ")\n",
    "dedupe_slice_full = phasor.Deduplicator(phasor.slice_vec_bands(data, start=0, end=10), **kwargs)\n",
    "dedupe_slices = [phasor.Deduplicator(phasor.slice_vec_bands(data, start=i, end=i + 1), **kwargs)\n",
    "                 for i in range(10)]\n",
    "dedupe_boolean = phasor.Deduplicator(dedupe_slices[0])\n",
    "for ds in dedupe_slices[1:5]:\n",
    "    dedupe_boolean.merge(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "radius = 0.8\n",
    "\n",
    "pairs_boolean = dedupe_boolean(radius)\n",
    "pairs_single = dedupe_slices[0](radius)\n",
    "pairs_full = dedupe_slice_full(radius)\n",
    "print(\"Number of candidates found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(pairs_boolean))\n",
    "print(\"Single-band test: \", len(pairs_single))\n",
    "print(\"Full-band test:   \", len(pairs_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_true_positives_boolean = set(\n",
    "    frozenset((a, b)) for a, b in pairs_boolean\n",
    "    if metadata['title'][a] == metadata['title'][b]\n",
    ")\n",
    "likely_true_positives_single = set(\n",
    "    frozenset((a, b)) for a, b in pairs_single\n",
    "    if metadata['title'][a] == metadata['title'][b]\n",
    ")\n",
    "likely_true_positives_full = set(\n",
    "    frozenset((a, b)) for a, b in pairs_full\n",
    "    if metadata['title'][a] == metadata['title'][b]\n",
    ")\n",
    "\n",
    "print(\"Number of likely duplicates (based on title) found by each test\")\n",
    "print()\n",
    "print(\"Boolean test:     \", len(likely_true_positives_boolean))\n",
    "print(\"Single-band test: \", len(likely_true_positives_single))\n",
    "print(\"Full-band test:   \", len(likely_true_positives_full))\n",
    "print()\n",
    "print(\"Number of identically-titled volumes missed by single test, caught by boolean test:\")\n",
    "print(len(likely_true_positives_boolean - likely_true_positives_single))\n",
    "print()\n",
    "print(\"Number of identically-titled volumes missed by boolean test, caught by single test:\")\n",
    "print(len(likely_true_positives_single - likely_true_positives_boolean))\n",
    "print()\n",
    "print(\"NOTE: Many false positives will still appear in these counts because \"\n",
    "      \"different volumes from multi-volume works may have the same title even \"\n",
    "      \"though they do not contain the same content. This accounts for many of the \"\n",
    "      \"matches captured by the single-band but not by the boolean test. The \"\n",
    "      \"single-band test captures the broad semantic similarity between volumes of \"\n",
    "      \"the same work, but can't make fine-grained distinctions between individual \"\n",
    "      \"volumes of the work. A single ten-volume work can produce as many as one \"\n",
    "      \"hundred false positives here, so this can give the impression that the \"\n",
    "      \"boolean test has missed many duplicates. Hand check a few and you'll \"\n",
    "      \"probably see that it hasn't.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(pairs_boolean)\n",
    "\n",
    "def get_root(links, ix):\n",
    "    while links[ix] != ix:\n",
    "        ix = links[ix]\n",
    "    return ix\n",
    "\n",
    "def set_root(links, ix, root):\n",
    "    while links[ix] != ix:\n",
    "        old_ix = ix\n",
    "        ix = links[ix]\n",
    "        links[old_ix] = root\n",
    "    links[ix] = root\n",
    "\n",
    "def cluster_pairs(pairs, maxn=None):\n",
    "    if maxn is None:\n",
    "        maxn = max(x for p in pairs for x in p) + 1\n",
    "        \n",
    "    dupe_link = list(range(maxn))\n",
    "    for a, b in pairs:\n",
    "        a_root = get_root(dupe_link, a)\n",
    "        set_root(dupe_link, a, a_root)\n",
    "        set_root(dupe_link, b, a_root)\n",
    "    \n",
    "    for i in range(len(dupe_link)):\n",
    "        dupe_link[i] = get_root(dupe_link, i)\n",
    "\n",
    "    dupe_clusters = {c: [] for c in dupe_link}\n",
    "    for i, c in enumerate(dupe_link):\n",
    "        dupe_clusters[c].append(i)\n",
    "\n",
    "    return list(dupe_clusters.values())\n",
    "\n",
    "dupe_clusters = cluster_pairs(pairs, len(data))\n",
    "print('Total number of items:', sum(len(c) for c in dupe_clusters))\n",
    "print('Number of clusters:', len(dupe_clusters))\n",
    "print('Largest cluster:', max(len(c) for c in dupe_clusters))\n",
    "print('Number of one-item clusters:', sum(len(c) == 1 for c in dupe_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_cluster(indices, data):\n",
    "    centroid = sum(data[i] for i in indices) / len(indices)\n",
    "    distances = [((data[i] - centroid) ** 2).sum() ** 0.5 for i in indices]\n",
    "    order = sorted(range(len(distances)), key=distances.__getitem__)\n",
    "    indices_sorted = [indices[o] for o in order]\n",
    "    distances = [distances[o] for o in order]\n",
    "    return list(zip(indices_sorted, distances))\n",
    "\n",
    "def mean_dist(cluster_dist):\n",
    "    return sum(d for c, d in cluster_dist) / len(cluster_dist)\n",
    "\n",
    "display_clusters = [sort_cluster(dc, data)\n",
    "                    for dc in dupe_clusters if len(dc) > 1]\n",
    "display_clusters.sort(key=mean_dist)\n",
    "\n",
    "for i, dc in enumerate(display_clusters):\n",
    "    print()\n",
    "    print(f'Cluster {i}, {len(dc)} items:')\n",
    "    for j, (vol, dist) in enumerate(dc):\n",
    "        vol_id = metadata.index[vol]\n",
    "        vol_ti = metadata['title'][vol]\n",
    "        vol_au = metadata['author'][vol]\n",
    "        print('    Item', j, ' ~~  distance from cluster centroid:', dist)\n",
    "        print('   ', vol_au, ' ~~ ', vol_ti)\n",
    "        print('   ', phasor.htid_url(vol_id))\n",
    "        print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_freq_array = [phasor.unflatten_vec(r) / len(data) for r in data]  # 300 rows, 20 cols in each array, ~1000 arrays\n",
    "\n",
    "data_freq_mean = data_freq_array[0]\n",
    "for dfa in data_freq_array[1:]:\n",
    "    data_freq_mean += dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_a = data_freq_mean[:, 1:].real\n",
    "power_b = data_freq_mean[:, 1:].imag\n",
    "power = (power_a * power_a + power_b * power_b) ** 0.5\n",
    "\n",
    "mean_power = power.sum(axis=0) / 300\n",
    "plt.plot(mean_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_mean = mean_power / mean_power.mean()\n",
    "scaled_power = power / power.mean(axis=0)\n",
    "# scaled_rel_diff = []\n",
    "\n",
    "scaled_diffs = numpy.array([(((scaled_power[i] - scaled_mean) / scaled_power[i]) ** 2).sum()\n",
    "                            for i in range(len(power))])\n",
    "\n",
    "scaled_diffs_argsort = scaled_diffs.argsort()\n",
    "\n",
    "for chunk in range(10):\n",
    "    for i in range(chunk * 30, chunk * 30 + 30):\n",
    "        plt.plot(power[scaled_diffs_argsort[i]] / \n",
    "                 power[scaled_diffs_argsort[i]].mean())\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstable_vec = numpy.zeros(len(scaled_diffs), dtype=numpy.float64)\n",
    "unstable_vec[scaled_diffs_argsort] = (numpy.arange(len(scaled_diffs)) / len(scaled_diffs)) > 0.99\n",
    "unstable_vec = unstable_vec.reshape(1, -1)\n",
    "\n",
    "stable_vec = numpy.zeros(len(scaled_diffs), dtype=numpy.float64)\n",
    "stable_vec[scaled_diffs_argsort] = (numpy.arange(len(scaled_diffs)) / len(scaled_diffs)) < 0.01\n",
    "stable_vec = stable_vec.reshape(1, -1)\n",
    "\n",
    "def get_similar(vec, comp_word):\n",
    "    vec = vec + phasor.en_nlp.vocab.vectors[phasor.en_nlp.vocab.strings[comp_word]]\n",
    "    t_id = phasor.en_nlp.vocab.vectors.most_similar(vec)[0][0]\n",
    "    return phasor.en_nlp.vocab.strings[t_id]\n",
    "\n",
    "# get_similar(unstable_vec, \"lost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
