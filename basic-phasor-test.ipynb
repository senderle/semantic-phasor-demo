{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import textwrap\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "import numpy\n",
    "import pandas\n",
    "import umap\n",
    "\n",
    "from headless import load_pages\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource\n",
    "from bokeh.palettes import magma\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load('en_core_web_md', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BANDS = 50\n",
    "\n",
    "def htid_url(htid):\n",
    "    htid = htid.replace('+', ':').replace('=', '/')\n",
    "    return 'https://babel.hathitrust.org/cgi/pt?id={}'.format(htid)\n",
    "\n",
    "def path_to_htid(path):\n",
    "    htid = os.path.split(path)[-1]\n",
    "    htid = os.path.splitext(htid)[0]\n",
    "    return htid.replace('+', ':').replace('=', '/')\n",
    "\n",
    "def volume_paths(path):\n",
    "    \"\"\"List all zip files and subfolders in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if os.path.isdir(f) or f.endswith('.zip')]\n",
    "\n",
    "def numpy_paths(path):\n",
    "    \"\"\"List all numpy files in the given folder.\"\"\"\n",
    "    files = (os.path.join(path, f) for f in sorted(os.listdir(path)))\n",
    "    return [f for f in files if f.endswith('.npy')]\n",
    "\n",
    "def load_one_sp_embedding(volume_path):\n",
    "    \"\"\"Parse the text of one volume and extract word vectors.\"\"\"\n",
    "    sp_text = en_nlp.pipe(load_pages(volume_path))\n",
    "    return numpy.array([tok.vector.reshape(-1) for doc in sp_text for tok in doc if not tok.is_space])\n",
    "\n",
    "def piecewise_avg(vec, n_groups):\n",
    "    \"\"\"Divide a vector into pieces and return the average for each.\"\"\"\n",
    "    size = len(vec) / n_groups\n",
    "    ends = []\n",
    "    for i in range(1, n_groups + 1):\n",
    "        ends.append(int(size * i))\n",
    "    ends[-1] = len(vec)\n",
    "    \n",
    "    sums = []\n",
    "    start = 0\n",
    "    for end in ends:\n",
    "        sums.append(vec[start:end].sum() / (end - start))\n",
    "        start = end\n",
    "        \n",
    "    return numpy.array(sums)\n",
    "        \n",
    "def embedding_fft(sp_embedding, n_bands=N_BANDS):\n",
    "    \"\"\"\n",
    "    Perform a Fourier transform on all the dimensions of an\n",
    "    array of word embeddings extracted from a document. \n",
    "    `sp_embedding` is assumed to be an array with a row\n",
    "    for each document, and a column for each dimension of \n",
    "    the underlying word embedding vector model.\n",
    "    \"\"\"\n",
    "    fft_cols = []\n",
    "    n_groups = 1\n",
    "    while n_groups < n_bands * 10:\n",
    "        n_groups *= 2\n",
    "        \n",
    "    for col in range(sp_embedding.shape[1]):\n",
    "        vec = sp_embedding[:, col]\n",
    "        vec = piecewise_avg(vec, n_groups)\n",
    "        fft = numpy.fft.rfft(vec)\n",
    "        fft_cols.append(fft[:n_bands])\n",
    "    \n",
    "    return numpy.array(fft_cols)\n",
    "\n",
    "def flatten_fft(emb_fft, start=0, end=None):\n",
    "    \"\"\"Reshape an fft array into a single vector.\"\"\"\n",
    "    complex_vec = numpy.array(emb_fft)[:, start:end].reshape(-1)\n",
    "    return numpy.array([x for r_i in zip(complex_vec.real, complex_vec.imag)\n",
    "                        for x in r_i])\n",
    "\n",
    "def unflatten_vec(doc_vector, cols=N_BANDS * 2):\n",
    "    \"\"\"Turn a document vector back into an fft array.\"\"\"\n",
    "    array = doc_vector.reshape(300, cols)  # This hard-codes values that should be parameters.\n",
    "    real = array[:, ::2]\n",
    "    imag = array[:, 1::2]\n",
    "    return real + imag * 1j\n",
    "    \n",
    "def test_fft_reshape(volume_path):\n",
    "    \"\"\"A test of vector-array conversion routines.\"\"\"\n",
    "    assert _test_fft_reshape_one(volume_path)\n",
    "\n",
    "def _test_fft_reshape_one(folder):\n",
    "    emb = load_one_sp_embedding(folder)\n",
    "    \n",
    "    fft_orig = embedding_fft(emb)\n",
    "    \n",
    "    fft_complex = unflatten_vec(flatten_fft(fft_orig))\n",
    "    return (fft_orig == fft_complex).all()\n",
    "\n",
    "def save_embedding_ffts(source_path, dest_path=None):\n",
    "    dest_path = source_path if dest_path is None else dest_path\n",
    "    vol_paths = volume_paths(source_path)\n",
    "    vol_paths = [os.path.split(vp)[-1] for vp in vol_paths]\n",
    "    vol_paths = [vp if not vp.endswith('.zip') else vp[:-4] for vp in vol_paths]\n",
    "    new_paths = [os.path.join(dest_path, vp) for vp in vol_paths]\n",
    "    \n",
    "    sp_embeddings = (load_one_sp_embedding(v) for v in vol_paths)\n",
    "    emb_ffts = (embedding_fft(e) for e in sp_embeddings)\n",
    "    for emb, np in zip(emb_ffts, new_paths):\n",
    "        numpy.save(np, emb) \n",
    "    \n",
    "def load_embedding_fft_array(path, start=0, end=None, \n",
    "                             reload=False, htid_test=None, _cache={}):\n",
    "    if (reload or not _cache or _cache['start'] != start or \n",
    "                _cache['end'] != end or htid_test is not None):\n",
    "        if htid_test is not None:\n",
    "            assert [path_to_htid(p) for p in numpy_paths(path)] == list(htid_test)\n",
    "        _cache['start'] = start\n",
    "        _cache['end'] = end\n",
    "        _cache['data'] = numpy.array([flatten_fft(numpy.load(f), start, end) \n",
    "                                      for f in numpy_paths(path)])  \n",
    "    return _cache['data']\n",
    "\n",
    "# def load_embedding_fft_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "def load_metadata(metadata_path, fft_path, csv_delim='\\t', htid_col='htid'):\n",
    "    ids = [path_to_htid(p)\n",
    "           for p in numpy_paths(fft_path)]\n",
    "    metadata = pandas.read_csv(metadata_path, index_col=htid_col, delimiter=csv_delim)\n",
    "    return metadata.reindex(ids, fill_value='[metadata missing]')\n",
    "\n",
    "def load_fft_metadata(fft_path, metadata_path, start=0, end=None, reload=False,\n",
    "                      csv_delim='\\t', htid_col='htid'):\n",
    "    metadata = load_metadata(metadata_path, fft_path, csv_delim, htid_col)\n",
    "    fft_arr = load_embedding_fft_array(fft_path, start, end, reload, metadata.index)\n",
    "    return fft_arr, metadata\n",
    "\n",
    "def show_dataset(folder, n=10):\n",
    "    volumes = volume_paths(folder)\n",
    "    for v in volumes:\n",
    "        print(load_pages(v)[0][0:500])\n",
    "\n",
    "def show_umap(data, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    plt.gca().axis('equal')\n",
    "    plt.scatter(vis[:, 0], \n",
    "                vis[:, 1], \n",
    "                c=[i / len(vis) for i in range(len(vis))],\n",
    "                cmap='plasma')\n",
    "    plt.show()\n",
    "\n",
    "def show_umap_bokeh(data, metadata, n_neighbors=20, min_dist=0.001, metric='euclidean'):\n",
    "    um = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, metric=metric)\n",
    "    vis = um.fit_transform(data)\n",
    "    magma_palette = magma(10)\n",
    "    authors = [a if isinstance(a, str) else 'missing author'\n",
    "               for a in metadata['author']]\n",
    "    au_rank = {au: i / len(authors) for i, au in enumerate(sorted(authors))}\n",
    "    color = [magma_palette[int(au_rank[a] * 10)] for a in authors]\n",
    "    scatter_data = pandas.DataFrame({'umap_1': vis[:, 0], \n",
    "                                     'umap_2': vis[:, 1],\n",
    "                                     'color': color,\n",
    "                                     'htid': list(metadata.index),\n",
    "                                     'title': ['<br>'.join(textwrap.wrap(t)) \n",
    "                                               for t in metadata['title']],\n",
    "                                     'author': list(metadata['author'])\n",
    "    })\n",
    "    \n",
    "    plot_figure = figure(\n",
    "        title='UMAP Projection of Phasor vectors for ~1000 random HathiTrust volumes',\n",
    "        plot_width=800,\n",
    "        plot_height=800,\n",
    "        tools=('pan, wheel_zoom, reset')\n",
    "    )\n",
    "\n",
    "    plot_figure.add_tools(HoverTool(\n",
    "        tooltips=(\"<div><span style='font-size: 10px'>@htid{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@author{safe}</span></div>\"\n",
    "                  \"<div><span style='font-size: 10px'>@title{safe}</span></div>\"\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    plot_figure.circle(\n",
    "        'umap_1',\n",
    "        'umap_2',\n",
    "        color='color',\n",
    "        source=scatter_data,\n",
    "    )\n",
    "\n",
    "    show(plot_figure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_volumes = '/media/secure_volume/workset/orig'\n",
    "# path_to_fft = '/media/secure_volume/workset/fft_npy'\n",
    "# path_to_meta = '../fiction-rand-penn-1072.tsv'\n",
    "path_to_volumes = '../ht-open-test-data/gov_docs/'\n",
    "path_to_fft = '../ht-open-test-data/gov_docs_fft'\n",
    "path_to_meta = '../ht-open-test-data/gov_docs_fakemeta.csv'\n",
    "# path_to_volumes = '../ht-open-test-data/fiction_998/'\n",
    "# path_to_fft = '../ht-open-test-data/fiction_fft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_path in volume_paths(path_to_volumes)[:3]:\n",
    "    print('testing with {}'.format(test_path))\n",
    "    try:\n",
    "        test_fft_reshape(test_path)\n",
    "    except Exception as e:\n",
    "        print('skipping {} -- {}: {}'.format(test_path, type(e), e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_embedding_ffts(path_to_volumes, path_to_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, metadata = load_fft_metadata(path_to_fft, path_to_meta)\n",
    "show_umap_bokeh(\n",
    "    data,\n",
    "    metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umap = umap.UMAP(n_neighbors=5, n_components=10, metric='euclidean').fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_umap_kd = KDTree(data_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(data_umap_kd.query_pairs(0.010))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_ids = [(metadata.index[x], metadata.index[y]) for x, y in pairs]\n",
    "distances = [((data_umap[x] - data_umap[y]) ** 2).sum() ** 0.5 for x, y in pairs]\n",
    "order = sorted(range(len(distances)), key=distances.__getitem__)\n",
    "pair_ids = [pair_ids[i] for i in order]\n",
    "distances = [distances[i] for i in order]\n",
    "\n",
    "for a, b in pairs:\n",
    "    a_id = metadata.index[a]\n",
    "    b_id = metadata.index[b]\n",
    "    a_ti = metadata['title'][a]\n",
    "    b_ti = metadata['title'][b]\n",
    "    a_au = metadata['author'][a]\n",
    "    b_au = metadata['author'][b]\n",
    "    d = ((data_umap[a] - data_umap[b]) ** 2).sum() ** 0.5\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print('10-D UMAP Distance: ', d)\n",
    "    print(htid_url(a_id))\n",
    "    print(a_au, \" ~~ \", a_ti)\n",
    "    print(htid_url(b_id))\n",
    "    print(b_au, \" ~~ \", b_ti)\n",
    "    print()\n",
    "    print('------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a, b in pair_ids:\n",
    "#     diff = data_dict[a] - data_dict[b]\n",
    "#     plt.plot(diff)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist between 10 smallest pairs in base space\n",
    "pair_dists = numpy.array([((data[x] - data[y]) ** 2).sum() ** 0.5 \n",
    "                          for x, y in pairs])\n",
    "smallest_ix = pair_dists.argsort()[:10]\n",
    "print(\"closest pairs in base space\")\n",
    "pprint(list(zip([pairs[i] for i in smallest_ix], pair_dists[smallest_ix])))\n",
    "\n",
    "# dist between pairs in umap space\n",
    "pair_umap_dists = numpy.array([((data_umap[x] - data_umap[y]) ** 2).sum() ** 0.5\n",
    "                               for x, y in pairs])\n",
    "smallest_ix = pair_umap_dists.argsort()[:10]\n",
    "print(\"closest pairs in umap space\")\n",
    "pprint(list(zip([pairs[i] for i in smallest_ix], pair_umap_dists[smallest_ix])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_freq_array = [unflatten_vec(r) / len(data) for r in data]  # 300 rows, 20 cols in each array, ~1000 arrays\n",
    "\n",
    "data_freq_mean = data_freq_array[0]\n",
    "for dfa in data_freq_array[1:]:\n",
    "    data_freq_mean += dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_a = data_freq_mean[:, 1:].real\n",
    "power_b = data_freq_mean[:, 1:].imag\n",
    "power = (power_a * power_a + power_b * power_b) ** 0.5\n",
    "\n",
    "mean_power = power.sum(axis=0) / 300\n",
    "plt.plot(mean_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_mean = mean_power / mean_power.mean()\n",
    "scaled_power = power / power.mean(axis=0)\n",
    "# scaled_rel_diff = []\n",
    "\n",
    "scaled_diffs = numpy.array([(((scaled_power[i] - scaled_mean) / scaled_power[i]) ** 2).sum()\n",
    "                            for i in range(len(power))])\n",
    "\n",
    "scaled_diffs_argsort = scaled_diffs.argsort()\n",
    "\n",
    "for chunk in range(10):\n",
    "    for i in range(chunk * 30, chunk * 30 + 30):\n",
    "        plt.plot(power[scaled_diffs_argsort[i]] / \n",
    "                 power[scaled_diffs_argsort[i]].mean())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
